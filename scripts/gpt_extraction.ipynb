{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4c1f6f5",
   "metadata": {},
   "source": [
    "# Connect to Chatgpt API to collect entities and relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b83dfc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "\n",
    "load_dotenv()  # loads variables from .env\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5415a321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here are five random fruits:\n",
      "\n",
      "1. Mango\n",
      "2. Kiwi\n",
      "3. Dragon fruit\n",
      "4. Blueberry\n",
      "5. Papaya\n"
     ]
    }
   ],
   "source": [
    "# Quick test: Ask model for 5 words\n",
    "resp = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Say 5 random fruits\"}],\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "print(resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b789a92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              row_id      doc_hash                         document_name  \\\n",
      "0  12406E12_ARTCULO1  1.240600e+16  LEY AMBIENTAL DE LA CIUDAD DE MÉXICO   \n",
      "1  12406E12_ARTCULO2  1.240600e+16  LEY AMBIENTAL DE LA CIUDAD DE MÉXICO   \n",
      "2  12406E12_ARTCULO3  1.240600e+16  LEY AMBIENTAL DE LA CIUDAD DE MÉXICO   \n",
      "3  12406E12_ARTCULO4  1.240600e+16  LEY AMBIENTAL DE LA CIUDAD DE MÉXICO   \n",
      "4  12406E12_ARTCULO5  1.240600e+16  LEY AMBIENTAL DE LA CIUDAD DE MÉXICO   \n",
      "\n",
      "  document_section_title                                               text  \n",
      "0             Artículo 1  º.- La presente Ley es reglamentaria de las di...  \n",
      "1             Artículo 2  º.- Se consideran de utilidad pública:\\n\\nI. E...  \n",
      "2             Artículo 3  º.- En todo lo no previsto en la presente Ley,...  \n",
      "3             Artículo 4  º.- Para los efectos de esta Ley, se utilizará...  \n",
      "4             Artículo 5  º.- Son autoridades en materia ambiental en la...  \n"
     ]
    }
   ],
   "source": [
    "# Load your Excel file\n",
    "df = pd.read_excel(\"/Users/alexa/Projects/cdmx_kg/Mexico_City/LEY_AMBIENTAL_DE_LA_CIUDAD_DE_MEXICO.xlsx\")\n",
    "df = df[['row_id','doc_hash','document_name', 'document_section_title', 'text']]\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b193abcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Firstly ask to identify entities. \n",
    "# Output is a csv with a art_name | mention | \n",
    "\n",
    "# Prompts\n",
    "def prompt_entities(batch):\n",
    "    text = \"\\n\\n\".join(\n",
    "        f\"ROW_ID: {row.row_id}\\n\"\n",
    "        f\"DOCUMENT: {row.document_name}\\n\"\n",
    "        f\"SECTION: {row.document_section_title}\\n\"\n",
    "        f\"TEXT: {row.text}\"\n",
    "        for row in batch.itertuples()\n",
    "    )\n",
    "    return f\"\"\"You are a specialized legal entity extraction system for Mexico City government documents. Your task is to be EXHAUSTIVE and find ALL government entities mentioned.\n",
    "\n",
    "CRITICAL: READ EVERY WORD CAREFULLY. Do not miss any government entity, no matter how briefly mentioned.\n",
    "\n",
    "TASK: Extract ALL government entities, institutions, and legal bodies from these legal texts.\n",
    "\n",
    "ENTITY TYPES TO IDENTIFY (scan for ALL of these):\n",
    "- Secretarías (ministries): Secretaría de..., SEDEMA, SEDUVI, SIBISO, STyFE, etc.\n",
    "- Alcaldías (municipal governments): Alcaldía..., any of the 16 alcaldías\n",
    "- Institutos and agencies: Instituto de..., Agencia de..., ADIP, etc.\n",
    "- Tribunales and courts: Tribunal..., Juzgado..., Corte..., etc.\n",
    "- Consejos and commissions: Consejo de..., Comisión de..., etc.\n",
    "- Universities and schools: Universidad de..., UACM, etc.\n",
    "- Procuradurías: Procuraduría..., PAOT, etc.\n",
    "- Federal entities: SEMARNAT, CONAGUA, PROFEPA, etc.\n",
    "- Any organization with official government role\n",
    "\n",
    "COMPREHENSIVE EXTRACTION RULES:\n",
    "1. Extract EXACTLY as written in the text (preserve case, accents, articles like \"la\", \"el\")\n",
    "2. Include full official names AND abbreviations when mentioned\n",
    "3. Look for entities in ALL parts of the text, including parentheses, footnotes, lists\n",
    "4. Capture entities mentioned in different forms (e.g., \"la Secretaría\", \"dicha Secretaría\")\n",
    "5. Include entities that appear in compound phrases\n",
    "6. Do NOT skip vague references - if it's a government body, include it\n",
    "7. Be especially careful with lists and enumerated items\n",
    "\n",
    "SEARCH STRATEGY:\n",
    "- Read the text word by word\n",
    "- Look for capital letters that might indicate proper nouns\n",
    "- Check for organizational keywords: Secretaría, Instituto, Consejo, Comisión, Alcaldía, etc.\n",
    "- Examine abbreviations in parentheses\n",
    "- Review any lists or bullet points carefully\n",
    "\n",
    "OUTPUT FORMAT - CSV rows only, no headers, no markdown blocks:\n",
    "row_id,mention\n",
    "\n",
    "EXAMPLES:\n",
    "12406E12_ARTCULO5,Secretaría del Medio Ambiente\n",
    "12406E12_ARTCULO5,SEDEMA\n",
    "12406E12_ARTCULO5,Jefatura de Gobierno de la Ciudad de México\n",
    "12406E12_ARTCULO5,la Secretaría\n",
    "12406E12_ARTCULO5,Alcaldía Benito Juárez\n",
    "\n",
    "INPUT:\n",
    "{text}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "344e21e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Secondly ask to identify article mentions. \n",
    "#Each article mention must include the number article and the law.\n",
    "# Output is a csv with art_name | art_mention \n",
    "\n",
    "def prompt_article_mentions(batch):\n",
    "    text = \"\\n\\n\".join(\n",
    "        f\"ROW_ID: {row.row_id}\\n\"\n",
    "        f\"DOCUMENT: {row.document_name}\\n\"\n",
    "        f\"SECTION: {row.document_section_title}\\n\"\n",
    "        f\"TEXT: {row.text}\"\n",
    "        for row in batch.itertuples()\n",
    "    )\n",
    "    return f\"\"\"\n",
    "Extract ALL laws and articles of laws mentions from Mexican law documents. Analyze ONLY the TEXT field.\n",
    "\n",
    "CRITICAL RULES:\n",
    "• ONLY extract actual references to LAWS, ARTICLES, or LEGAL DOCUMENTS. \n",
    "• DO NOT extract mere mentions of government entities, authorities, or institutions.\n",
    "• For \"artículo anterior\": use current article number -1, same law\n",
    "• For \"esta Ley\"/\"presente Ley\": use law name from DOCUMENT field\n",
    "• For article WITHOUT law mentioned: assume it refers to current law from DOCUMENT field\n",
    "• For \"reglamento\": format as \"REGLAMENTO DE + [DOCUMENT]\"\n",
    "\n",
    "\n",
    "EXTRACT:\n",
    "• Articles: \"artículo 123\", \"art. 45\", \"artículos 1 al 15\"\n",
    "• Laws: \"Ley de...\", \"Código Civil\", \"Constitución Política\", \"Reglamento de...\"\n",
    "• Self-references: \"esta Ley\", \"la presente Ley\"\n",
    "• Relative refs: \"artículo anterior\", \"artículo siguiente\"\n",
    "\n",
    "RULES:\n",
    "1. One row per mention (split ranges: \"artículos 13 y 16\" = 2 rows)\n",
    "2. Include complete phrase in mention_extraction and 10 words before and after\n",
    "3. For ranges/multiple: create separate rows\n",
    "4. Check parentheses, lists, subordinate clauses\n",
    "5. Law names can be long with commas (e.g., \"LEY DEL DERECHO AL ACCESO, DISPOSICIÓN Y SANEAMIENTO DEL AGUA DE LA CIUDAD DE MÉXICO\")\n",
    "\n",
    "\n",
    "OUTPUT (CSV only, no headers):\n",
    "row_id,art_num,law_name,mention_extraction\n",
    "\n",
    "COLUMN DEFINITIONS:\n",
    "• row_id: The ROW_ID of the current article being analyzed (from input)\n",
    "• art_num: Article number being referenced (leave EMPTY if no article number)\n",
    "• law_name: Name of the law/document containing the referenced article\n",
    "• mention_extraction: Complete text phrase of the legal mention and 10 words before and after as it appears in the text\n",
    "\n",
    "EXAMPLES:\n",
    "Text: \"artículo 13 de la Constitución Política\" → 12406E12_ARTCULO1,13,CONSTITUCIÓN POLÍTICA,artículo 13 de la Constitución Política\n",
    "Text: \"el Código Civil\" → 12406E12_ARTCULO3,,CÓDIGO CIVIL,el Código Civil  \n",
    "Text: \"artículo anterior\" (in ARTÍCULO 5) → 12406E12_ARTCULO5,4,LEY AMBIENTAL DE LA CIUDAD DE MÉXICO,artículo anterior\n",
    "Text: \"artículo 10\" (no law mentioned, current law is LEY AMBIENTAL) → 12406E12_ARTCULO5,10,LEY AMBIENTAL DE LA CIUDAD DE MÉXICO,artículo 10\n",
    "Text: \"su reglamento\" (current law is LEY AMBIENTAL) → 12406E12_ARTCULO5,,REGLAMENTO DE LA LEY AMBIENTAL DE LA CIUDAD DE MÉXICO,su reglamento\n",
    "\n",
    "INPUT:\n",
    "{text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64496c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 332 articles in 42 batches...\n",
      "\n",
      "Processing batch 1/42 (articles 1-8)\n",
      "  → Extracting article mentions...\n",
      "    Found 20 unique mentions (from 23 total)\n",
      "\n",
      "Processing batch 2/42 (articles 9-16)\n",
      "  → Extracting article mentions...\n",
      "    Found 11 unique mentions (from 16 total)\n",
      "\n",
      "Processing batch 3/42 (articles 17-24)\n",
      "  → Extracting article mentions...\n",
      "    Found 8 unique mentions (from 16 total)\n",
      "\n",
      "Processing batch 4/42 (articles 25-32)\n",
      "  → Extracting article mentions...\n",
      "    Found 35 unique mentions (from 43 total)\n",
      "\n",
      "Processing batch 5/42 (articles 33-40)\n",
      "  → Extracting article mentions...\n",
      "    Found 17 unique mentions (from 20 total)\n",
      "\n",
      "Processing batch 6/42 (articles 41-48)\n",
      "  → Extracting article mentions...\n",
      "    Found 15 unique mentions (from 17 total)\n",
      "\n",
      "Processing batch 7/42 (articles 49-56)\n",
      "  → Extracting article mentions...\n",
      "    Found 13 unique mentions (from 19 total)\n",
      "\n",
      "Processing batch 8/42 (articles 57-64)\n",
      "  → Extracting article mentions...\n",
      "    Found 13 unique mentions (from 21 total)\n",
      "\n",
      "Processing batch 9/42 (articles 65-72)\n",
      "  → Extracting article mentions...\n",
      "    Found 7 unique mentions (from 14 total)\n",
      "\n",
      "Processing batch 10/42 (articles 73-80)\n",
      "  → Extracting article mentions...\n",
      "    Found 9 unique mentions (from 16 total)\n",
      "\n",
      "Processing batch 11/42 (articles 81-88)\n",
      "  → Extracting article mentions...\n",
      "    Found 48 unique mentions (from 52 total)\n",
      "\n",
      "Processing batch 12/42 (articles 89-96)\n",
      "  → Extracting article mentions...\n",
      "    Found 14 unique mentions (from 14 total)\n",
      "\n",
      "Processing batch 13/42 (articles 97-104)\n",
      "  → Extracting article mentions...\n",
      "    Found 7 unique mentions (from 14 total)\n",
      "\n",
      "Processing batch 14/42 (articles 105-112)\n",
      "  → Extracting article mentions...\n",
      "    Found 10 unique mentions (from 15 total)\n",
      "\n",
      "Processing batch 15/42 (articles 113-120)\n",
      "  → Extracting article mentions...\n",
      "    Found 8 unique mentions (from 16 total)\n",
      "\n",
      "Processing batch 16/42 (articles 121-128)\n",
      "  → Extracting article mentions...\n",
      "    Found 10 unique mentions (from 16 total)\n",
      "\n",
      "Processing batch 17/42 (articles 129-136)\n",
      "  → Extracting article mentions...\n",
      "    Found 10 unique mentions (from 16 total)\n",
      "\n",
      "Processing batch 18/42 (articles 137-144)\n",
      "  → Extracting article mentions...\n",
      "    Found 14 unique mentions (from 15 total)\n",
      "\n",
      "Processing batch 19/42 (articles 145-152)\n",
      "  → Extracting article mentions...\n",
      "    Found 16 unique mentions (from 21 total)\n",
      "\n",
      "Processing batch 20/42 (articles 153-160)\n",
      "  → Extracting article mentions...\n",
      "    Found 8 unique mentions (from 16 total)\n",
      "\n",
      "Processing batch 21/42 (articles 161-168)\n",
      "  → Extracting article mentions...\n",
      "    Found 8 unique mentions (from 16 total)\n",
      "\n",
      "Processing batch 22/42 (articles 169-176)\n",
      "  → Extracting article mentions...\n",
      "    Found 13 unique mentions (from 19 total)\n",
      "\n",
      "Processing batch 23/42 (articles 177-184)\n",
      "  → Extracting article mentions...\n",
      "    Found 11 unique mentions (from 16 total)\n",
      "\n",
      "Processing batch 24/42 (articles 185-192)\n",
      "  → Extracting article mentions...\n",
      "    Found 7 unique mentions (from 14 total)\n",
      "\n",
      "Processing batch 25/42 (articles 193-200)\n",
      "  → Extracting article mentions...\n",
      "    Found 8 unique mentions (from 16 total)\n",
      "\n",
      "Processing batch 26/42 (articles 201-208)\n",
      "  → Extracting article mentions...\n",
      "    Found 9 unique mentions (from 16 total)\n",
      "\n",
      "Processing batch 27/42 (articles 209-216)\n",
      "  → Extracting article mentions...\n"
     ]
    }
   ],
   "source": [
    "# Helper function to clean and validate GPT output\n",
    "def clean_csv_output(raw_output, expected_columns):\n",
    "    \"\"\"Clean and validate CSV output from GPT, filtering out artifacts and malformed entries\"\"\"\n",
    "    clean_lines = []\n",
    "    for line in raw_output.strip().splitlines():\n",
    "        line = line.strip()\n",
    "        \n",
    "        # Skip empty lines\n",
    "        if not line:\n",
    "            continue\n",
    "            \n",
    "        # Skip markdown code blocks\n",
    "        if line.startswith('```'):\n",
    "            continue\n",
    "            \n",
    "        # Skip headers (case insensitive)\n",
    "        if line.lower().startswith('art_name') or line.lower().startswith('document'):\n",
    "            continue\n",
    "            \n",
    "        # Skip lines that look like instructions or comments\n",
    "        if line.startswith('#') or line.startswith('//') or line.startswith('OUTPUT') or line.startswith('EXAMPLES'):\n",
    "            continue\n",
    "            \n",
    "        # Must contain commas for CSV format\n",
    "        if ',' not in line:\n",
    "            continue\n",
    "            \n",
    "        # Check if line has the expected number of columns (allow some flexibility)\n",
    "        parts = line.split(',')\n",
    "        if len(parts) < expected_columns:\n",
    "            print(f\"    Skipping malformed line (too few columns): {line[:50]}...\")\n",
    "            continue\n",
    "            \n",
    "        # Clean each part\n",
    "        cleaned_parts = []\n",
    "        for part in parts[:expected_columns]:  # Only take expected number of columns\n",
    "            cleaned_part = part.strip().strip('\"').strip(\"'\")  # Remove quotes and extra spaces\n",
    "            cleaned_parts.append(cleaned_part)\n",
    "            \n",
    "        # Skip if any essential fields are empty (first two columns should not be empty)\n",
    "        if not cleaned_parts[0]:\n",
    "            print(f\"    Skipping line with empty essential fields: {line[:50]}...\")\n",
    "            continue\n",
    "            \n",
    "        # Validate row_id structure (should match pattern like: 12406E12_ARTCULO5)\n",
    "        import re\n",
    "        row_id_pattern = r'^[A-F0-9]{8}_[A-ZÁÉÍÓÚÑÜ]+\\d*$'\n",
    "        if not re.match(row_id_pattern, cleaned_parts[0]):\n",
    "            print(f\"    Skipping line with invalid row_id structure: {line[:50]}...\")\n",
    "            continue\n",
    "            \n",
    "        clean_lines.append(cleaned_parts)\n",
    "    \n",
    "    return clean_lines\n",
    "\n",
    "# Collectors\n",
    "gov_entities = []\n",
    "article_mentions = []\n",
    "\n",
    "# Batch size - reduced for better quality and completeness\n",
    "BATCH_SIZE = 8  # Even smaller batches for better attention\n",
    "\n",
    "total_batches = (len(df) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "print(f\"Processing {len(df)} articles in {total_batches} batches...\")\n",
    "\n",
    "for i, start in enumerate(range(0, len(df), BATCH_SIZE)):\n",
    "    batch = df.iloc[start:start+BATCH_SIZE]\n",
    "    batch_num = i + 1\n",
    "    \n",
    "    print(f\"\\nProcessing batch {batch_num}/{total_batches} (articles {start+1}-{min(start+BATCH_SIZE, len(df))})\")\n",
    "\n",
    "    # # Step 1 – Entities (with dual-pass for completeness)\n",
    "    # print(\"  → Extracting government entities...\")\n",
    "    # try:\n",
    "    #     # First pass - standard extraction\n",
    "    #     resp1 = client.chat.completions.create(\n",
    "    #         model=\"gpt-4o-mini\",\n",
    "    #         messages=[{\"role\": \"user\", \"content\": prompt_entities(batch)}],\n",
    "    #         temperature=0.1\n",
    "    #     )\n",
    "    #     output1 = resp1.choices[0].message.content.strip()\n",
    "        \n",
    "    #     # Second pass - with different temperature for variation\n",
    "    #     resp1_alt = client.chat.completions.create(\n",
    "    #         model=\"gpt-4o-mini\",\n",
    "    #         messages=[{\"role\": \"user\", \"content\": prompt_entities(batch)}],\n",
    "    #         temperature=0.3  # Higher temperature for different perspective\n",
    "    #     )\n",
    "    #     output1_alt = resp1_alt.choices[0].message.content.strip()\n",
    "        \n",
    "    #     # Combine both outputs\n",
    "    #     combined_entities = []\n",
    "    #     if output1:\n",
    "    #         cleaned_entities_1 = clean_csv_output(output1, 2)\n",
    "    #         combined_entities.extend(cleaned_entities_1)\n",
    "    #     if output1_alt:\n",
    "    #         cleaned_entities_2 = clean_csv_output(output1_alt, 2)\n",
    "    #         combined_entities.extend(cleaned_entities_2)\n",
    "        \n",
    "    #     # Remove duplicates while preserving order\n",
    "    #     seen = set()\n",
    "    #     unique_entities = []\n",
    "    #     for entity in combined_entities:\n",
    "    #         entity_key = tuple(entity)\n",
    "    #         if entity_key not in seen:\n",
    "    #             seen.add(entity_key)\n",
    "    #             unique_entities.append(entity)\n",
    "        \n",
    "    #     gov_entities.extend(unique_entities)\n",
    "    #     print(f\"    Found {len(unique_entities)} unique entities (from {len(combined_entities)} total)\")\n",
    "            \n",
    "    # except Exception as e:\n",
    "    #     print(f\"    Error extracting entities: {e}\")\n",
    "\n",
    "    # Step 2 – Article mentions (with dual-pass for completeness)\n",
    "    print(\"  → Extracting article mentions...\")\n",
    "    try:\n",
    "        # First pass - standard extraction\n",
    "        resp2 = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt_article_mentions(batch)}],\n",
    "            temperature=0.1\n",
    "        )\n",
    "        output2 = resp2.choices[0].message.content.strip()\n",
    "        \n",
    "        # Second pass - with different temperature for variation\n",
    "        resp2_alt = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt_article_mentions(batch)}],\n",
    "            temperature=0.3  # Higher temperature for different perspective\n",
    "        )\n",
    "        output2_alt = resp2_alt.choices[0].message.content.strip()\n",
    "        \n",
    "        # Combine both outputs\n",
    "        combined_mentions = []\n",
    "        if output2:\n",
    "            cleaned_mentions_1 = clean_csv_output(output2, 4)\n",
    "            combined_mentions.extend(cleaned_mentions_1)\n",
    "        if output2_alt:\n",
    "            cleaned_mentions_2 = clean_csv_output(output2_alt, 4)\n",
    "            combined_mentions.extend(cleaned_mentions_2)\n",
    "        \n",
    "        # Remove duplicates while preserving order\n",
    "        seen = set()\n",
    "        unique_mentions = []\n",
    "        for mention in combined_mentions:\n",
    "            mention_key = tuple(mention)\n",
    "            if mention_key not in seen:\n",
    "                seen.add(mention_key)\n",
    "                unique_mentions.append(mention)\n",
    "        \n",
    "        article_mentions.extend(unique_mentions)\n",
    "        print(f\"    Found {len(unique_mentions)} unique mentions (from {len(combined_mentions)} total)\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"    Error extracting mentions: {e}\")\n",
    "    \n",
    "    # Small delay between batches to avoid rate limits (now with 4 API calls per batch)\n",
    "    import time\n",
    "    time.sleep(0.5)\n",
    "\n",
    "print(f\"\\n=== EXTRACTION COMPLETE ===\")\n",
    "print(f\"Total government entities found: {len(gov_entities)}\")\n",
    "print(f\"Total article mentions found: {len(article_mentions)}\")\n",
    "\n",
    "# Quality summary\n",
    "print(f\"\\n=== QUALITY METRICS ===\")\n",
    "if gov_entities:\n",
    "    unique_entity_mentions = len(set(tuple(e) for e in gov_entities))\n",
    "    print(f\"Unique government entities: {unique_entity_mentions}\")\n",
    "if article_mentions:\n",
    "    unique_article_mentions = len(set(tuple(m) for m in article_mentions))\n",
    "    print(f\"Unique article mentions: {unique_article_mentions}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b122f074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved article mentions to article_mentions.csv\n"
     ]
    }
   ],
   "source": [
    "# Save results with proper validation\n",
    "# if gov_entities:\n",
    "#     entities_df = pd.DataFrame(gov_entities, columns=[\"row_id\", \"mention\"])\n",
    "#     entities_df.to_csv(\"gov_entities.csv\", index=False, encoding='utf-8')\n",
    "#     print(f\"✅ Saved government entities to gov_entities.csv\")\n",
    "# else:\n",
    "#     print(\"⚠️  No government entities to save\")\n",
    "\n",
    "if article_mentions:\n",
    "    mentions_df = pd.DataFrame(article_mentions, columns=[\"row_id\", \"art_num\", \"law_name\", \"mention_extraction\"])\n",
    "    mentions_df.to_csv(\"/Users/alexa/Projects/cdmx_kg/data/article_mentions_gpt.csv\", index=False, encoding='utf-8')\n",
    "    print(f\"✅ Saved article mentions to article_mentions.csv\")\n",
    "else:\n",
    "    print(\"⚠️  No article mentions to save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed2f96f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3555fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the entity normalization and ID system\n",
    "mentions_df, entities_df, normalization_map = create_entity_id_system()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd81a77",
   "metadata": {},
   "source": [
    "# This code section is to use the output of of the identified mentions and classfy them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a740460d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For government entities: \n",
    "# Read the dictionary of the document.\n",
    "# Then, go to the list of entities and check if it is needed to change the mention name based on the dictionary.\n",
    "# Replace the name if needed.\n",
    "# Not all the mentions will be needed to be changed, only ones specifiqued for each document. \n",
    "\n",
    "# Then, look at the list of mentions and create a new list with the id_hash for each differnte entity.\n",
    "# Add the hash_id to the list of mentions based on the entity name. \n",
    "# The output is the list of mentions with the hash_id for each entity and the list of unique entities with the hash_id.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4d07b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdmx_kg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
