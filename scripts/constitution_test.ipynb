{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "878aa99a",
   "metadata": {},
   "source": [
    "# Entity Recognition Pipeline for Mexico City Environmental Law\n",
    "\n",
    "This script processes legal texts to identify:\n",
    "1. Article mentions with expanded context (30 words before/after)\n",
    "2. Government entities and legal references using regex patterns only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c030e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import hashlib\n",
    "from collections import defaultdict\n",
    "from cdmx_entity_patterns_fixed import GOV_ENTITY_REGEX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cfb475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ID generation functions created\n"
     ]
    }
   ],
   "source": [
    "# ID Generation Functions\n",
    "def create_document_hash(document_name):\n",
    "    \"\"\"Create a short 8-character hash from document name\"\"\"\n",
    "    if pd.isna(document_name):\n",
    "        return \"UNKNOWN\"\n",
    "    # Create MD5 hash and take first 8 characters\n",
    "    hash_obj = hashlib.md5(str(document_name).encode('utf-8'))\n",
    "    return hash_obj.hexdigest()[:8].upper()\n",
    "\n",
    "def generate_scalable_row_id(row_index, document_name, section_title=\"\", prefix=\"CDMX\"):\n",
    "    \"\"\"Generate scalable IDs for multiple legal documents\"\"\"\n",
    "    doc_hash = create_document_hash(document_name)\n",
    "    section_clean = re.sub(r'[^\\w\\d]', '', str(section_title))[:6].upper() if section_title else \"NOSEC\"\n",
    "    return f\"{prefix}_{doc_hash}_{section_clean}_{row_index:05d}\"\n",
    "\n",
    "def generate_extraction_id(extraction_type, doc_hash, sequential_num):\n",
    "    \"\"\"Generate IDs for extracted data (articles, entities)\"\"\"\n",
    "    return f\"{extraction_type}_{doc_hash}_{sequential_num:05d}\"\n",
    "\n",
    "print(\"‚úì ID generation functions created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e34c1475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 332 rows\n",
      "Columns: ['city', 'type_document', 'type_document_2', 'juridiction_level ', 'juridiction_level_name', 'document_name', 'document_start_date', 'document_finished_date', 'document_section_title', 'text']\n",
      "\n",
      "üìù Pre-calculating ID components (EFFICIENT APPROACH)...\n",
      "Document hash for 'LEY AMBIENTAL DE LA CIUDAD DE M√âXICO': 12406E12\n",
      "‚úÖ ID components calculated once - much more efficient!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>doc_hash</th>\n",
       "      <th>section_clean</th>\n",
       "      <th>document_section_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12406E12_ARTCULO1</td>\n",
       "      <td>12406E12</td>\n",
       "      <td>ARTCULO1</td>\n",
       "      <td>Art√≠culo 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12406E12_ARTCULO2</td>\n",
       "      <td>12406E12</td>\n",
       "      <td>ARTCULO2</td>\n",
       "      <td>Art√≠culo 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12406E12_ARTCULO3</td>\n",
       "      <td>12406E12</td>\n",
       "      <td>ARTCULO3</td>\n",
       "      <td>Art√≠culo 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12406E12_ARTCULO4</td>\n",
       "      <td>12406E12</td>\n",
       "      <td>ARTCULO4</td>\n",
       "      <td>Art√≠culo 4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12406E12_ARTCULO5</td>\n",
       "      <td>12406E12</td>\n",
       "      <td>ARTCULO5</td>\n",
       "      <td>Art√≠culo 5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              row_id  doc_hash section_clean document_section_title\n",
       "0  12406E12_ARTCULO1  12406E12      ARTCULO1             Art√≠culo 1\n",
       "1  12406E12_ARTCULO2  12406E12      ARTCULO2             Art√≠culo 2\n",
       "2  12406E12_ARTCULO3  12406E12      ARTCULO3             Art√≠culo 3\n",
       "3  12406E12_ARTCULO4  12406E12      ARTCULO4             Art√≠culo 4\n",
       "4  12406E12_ARTCULO5  12406E12      ARTCULO5             Art√≠culo 5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the environmental law data\n",
    "ley_ambiental = pd.read_excel('/Users/alexa/Projects/cdmx_kg/Mexico_City/LEY_AMBIENTAL_DE_LA_CIUDAD_DE_MEXICO.xlsx')\n",
    "print(f\"Loaded {len(ley_ambiental)} rows\")\n",
    "print(f\"Columns: {ley_ambiental.columns.tolist()}\")\n",
    "\n",
    "print(\"\\nüìù Pre-calculating ID components (EFFICIENT APPROACH)...\")\n",
    "\n",
    "# 1. Add document hash column (calculate once per document)\n",
    "ley_ambiental['doc_hash'] = ley_ambiental['document_name'].apply(create_document_hash)\n",
    "\n",
    "# 2. Add cleaned section column\n",
    "ley_ambiental['section_clean'] = ley_ambiental['document_section_title'].apply(\n",
    "    lambda x: re.sub(r'[^a-zA-Z0-9]', '', str(x))[:13].upper() if pd.notna(x) else \"NOSEC\"\n",
    "    #lambda x: re.sub(r'[^\\w\\d]', '', str(x))[:13].upper() if pd.notna(x) else \"NOSEC\"\n",
    ")\n",
    "\n",
    "# 3. Generate final row IDs using pre-calculated components\n",
    "ley_ambiental['row_id'] = ley_ambiental.apply(\n",
    "    lambda row: f\"{row['doc_hash']}_{row['section_clean']}\", axis=1\n",
    ")\n",
    "\n",
    "# Show document hash for reference\n",
    "doc_hash = ley_ambiental['doc_hash'].iloc[0]\n",
    "print(f\"Document hash for '{ley_ambiental['document_name'].iloc[0]}': {doc_hash}\")\n",
    "print(f\"‚úÖ ID components calculated once - much more efficient!\")\n",
    "\n",
    "ley_ambiental[['row_id', 'doc_hash', 'section_clean', 'document_section_title']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9941ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Article extraction function updated with 30-word context\n"
     ]
    }
   ],
   "source": [
    "# def extract_article_mentions_extended(df, text_column='text', section_column='document_section_title'):\n",
    "#     \"\"\"\n",
    "#     Extract article mentions with 30 words before and after context.\n",
    "#     Returns only the full_context column for manual review.\n",
    "#     \"\"\"\n",
    "#     # Pattern to match article mentions in Spanish\n",
    "#     article_pattern = r'\\b(?:art[√≠i]culos?|art\\.?)\\s*(\\d+(?:\\s*[¬∞¬∫])?(?:\\s*bis|ter|qu[a√°]ter)?)\\b'\n",
    "    \n",
    "#     results = []\n",
    "#     counter = 1\n",
    "    \n",
    "#     for idx, row in df.iterrows():\n",
    "#         text = str(row[text_column]) if pd.notna(row[text_column]) else \"\"\n",
    "#         section_title = row[section_column] if pd.notna(row[section_column]) else \"\"\n",
    "        \n",
    "#         # Use pre-calculated document hash (much more efficient!)\n",
    "#         doc_hash = row.get('doc_hash', 'UNKNOWN')\n",
    "        \n",
    "#         # Find all matches in the text\n",
    "#         matches = list(re.finditer(article_pattern, text, re.IGNORECASE))\n",
    "        \n",
    "#         for match in matches:\n",
    "#             # Get the full matched text\n",
    "#             matched_text = match.group(0)\n",
    "            \n",
    "#             # Split text into words for context extraction\n",
    "#             words = text.split()\n",
    "            \n",
    "#             # Find the position of the match in the word list\n",
    "#             match_start_char = match.start()\n",
    "#             match_end_char = match.end()\n",
    "            \n",
    "#             # Find which words contain the match\n",
    "#             char_count = 0\n",
    "#             start_word_idx = 0\n",
    "#             end_word_idx = 0\n",
    "            \n",
    "#             for i, word in enumerate(words):\n",
    "#                 word_start = char_count\n",
    "#                 word_end = char_count + len(word)\n",
    "                \n",
    "#                 if word_start <= match_start_char <= word_end:\n",
    "#                     start_word_idx = i\n",
    "#                 if word_start <= match_end_char <= word_end:\n",
    "#                     end_word_idx = i\n",
    "#                     break\n",
    "                \n",
    "#                 char_count += len(word) + 1  # +1 for space\n",
    "            \n",
    "#             # Get 30 words before and after (expanded from 10)\n",
    "#             context_start = max(0, start_word_idx - 30)\n",
    "#             context_end = min(len(words), end_word_idx + 31)\n",
    "            \n",
    "#             words_before = words[context_start:start_word_idx]\n",
    "#             words_after = words[end_word_idx + 1:context_end]\n",
    "            \n",
    "#             context_before = \" \".join(words_before)\n",
    "#             context_after = \" \".join(words_after)\n",
    "            \n",
    "#             # Only save the full context with highlighted match\n",
    "#             full_context = f\"{context_before} **{matched_text}** {context_after}\"\n",
    "            \n",
    "#             results.append({\n",
    "#                 'article_id': generate_extraction_id('ART', doc_hash, counter),\n",
    "#                 'source_row_id': row.get('row_id', ''),\n",
    "#                 section_column: section_title,\n",
    "#                 'full_context': full_context\n",
    "#             })\n",
    "#             counter += 1\n",
    "    \n",
    "#     return pd.DataFrame(results)\n",
    "\n",
    "# print(\"‚úì Article extraction function updated with 30-word context\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47366049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FUNCI√ìN MEJORADA DE ART√çCULOS INTEGRADA\n",
      "   - Detecta m√∫ltiples art√≠culos: 'art√≠culos 50 y 325'\n",
      "   - Contexto dual: 30 palabras + 300 caracteres\n",
      "   - 3 patrones diferentes para mayor cobertura\n"
     ]
    }
   ],
   "source": [
    "def extract_article_mentions_improved(df, text_column='text', section_column='document_section_title'):\n",
    "    \"\"\"\n",
    "    VERSI√ìN MEJORADA: Detecta m√∫ltiples art√≠culos y ofrece contexto flexible\n",
    "    \n",
    "    MEJORAS:\n",
    "    1. ‚úÖ Detecta \"art√≠culos 50 y 325\" \n",
    "    2. ‚úÖ Contexto por caracteres (300) adem√°s de palabras (30)\n",
    "    3. ‚úÖ M√∫ltiples patrones para casos complejos\n",
    "    \"\"\"\n",
    "    \n",
    "    # PATRONES MEJORADOS\n",
    "    article_patterns = [\n",
    "        # Patr√≥n 1: M√∫ltiples art√≠culos con conectores\n",
    "        r'\\b(?:art[√≠i]culos?|art\\.?)\\s*\\d+(?:\\s*[¬∞¬∫])?(?:\\s*bis|ter|qu[a√°]ter)?(?:\\s*(?:y|al|,)\\s*\\d+(?:\\s*[¬∞¬∫])?(?:\\s*bis|ter|qu[a√°]ter)?)*',\n",
    "        \n",
    "        # Patr√≥n 2: Rangos \"del 10 al 15\"  \n",
    "        r'\\b(?:art[√≠i]culos?|art\\.?)\\s*(?:del\\s*)?\\d+(?:\\s*[¬∞¬∫])?(?:\\s*bis|ter|qu[a√°]ter)?\\s*al\\s*\\d+(?:\\s*[¬∞¬∫])?(?:\\s*bis|ter|qu[a√°]ter)?',\n",
    "        \n",
    "        # Patr√≥n 3: Individual (respaldo)\n",
    "        r'\\b(?:art[√≠i]culos?|art\\.?)\\s*\\d+(?:\\s*[¬∞¬∫])?(?:\\s*bis|ter|qu[a√°]ter)?'\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    counter = 1\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        text = str(row[text_column]) if pd.notna(row[text_column]) else \"\"\n",
    "        section_title = row[section_column] if pd.notna(row[section_column]) else \"\"\n",
    "        doc_hash = row.get('doc_hash', 'UNKNOWN')\n",
    "        \n",
    "        if not text.strip():\n",
    "            continue\n",
    "            \n",
    "        # Aplicar patrones (evita duplicados con set)\n",
    "        found_matches = set()\n",
    "        \n",
    "        for pattern_idx, pattern in enumerate(article_patterns):\n",
    "            matches = re.finditer(pattern, text, re.IGNORECASE)\n",
    "            \n",
    "            for match in matches:\n",
    "                match_key = (match.start(), match.end(), match.group(0))\n",
    "                if match_key in found_matches:\n",
    "                    continue\n",
    "                found_matches.add(match_key)\n",
    "                \n",
    "                matched_text = match.group(0)\n",
    "                \n",
    "                # CONTEXTO MEJORADO: 30 palabras + 300 caracteres\n",
    "                context_words = get_word_context(text, match, 30)\n",
    "                context_chars = get_char_context(text, match, 300)\n",
    "                \n",
    "                results.append({\n",
    "                    'article_id': f\"ART_{doc_hash}_{counter:05d}\",\n",
    "                    'source_row_id': row.get('row_id', ''),\n",
    "                    'pattern_type': f\"Pattern_{pattern_idx + 1}\",\n",
    "                    section_column: section_title,\n",
    "                    'matched_text': matched_text,\n",
    "                    'context_30_words': context_words,\n",
    "                    'context_300_chars': context_chars,\n",
    "                    'start_char': match.start(),\n",
    "                    'end_char': match.end(),\n",
    "                })\n",
    "                counter += 1\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def get_word_context(text, match, num_words):\n",
    "    \"\"\"Contexto por palabras\"\"\"\n",
    "    words = text.split()\n",
    "    text_before_match = text[:match.start()]\n",
    "    text_after_match = text[match.end():]\n",
    "    \n",
    "    words_before = text_before_match.split()[-num_words:] if text_before_match else []\n",
    "    words_after = text_after_match.split()[:num_words] if text_after_match else []\n",
    "    \n",
    "    before = \" \".join(words_before)\n",
    "    after = \" \".join(words_after)\n",
    "    \n",
    "    return f\"{before} **{match.group(0)}** {after}\".strip()\n",
    "\n",
    "def get_char_context(text, match, num_chars):\n",
    "    \"\"\"Contexto por caracteres\"\"\"\n",
    "    start = max(0, match.start() - num_chars)\n",
    "    end = min(len(text), match.end() + num_chars)\n",
    "    \n",
    "    before = text[start:match.start()]\n",
    "    after = text[match.end():end]\n",
    "    \n",
    "    return f\"{before}**{match.group(0)}**{after}\"\n",
    "\n",
    "print(\"‚úÖ FUNCI√ìN MEJORADA DE ART√çCULOS INTEGRADA\")\n",
    "print(\"   - Detecta m√∫ltiples art√≠culos: 'art√≠culos 50 y 325'\")\n",
    "print(\"   - Contexto dual: 30 palabras + 300 caracteres\")\n",
    "print(\"   - 3 patrones diferentes para mayor cobertura\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76e20d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW organized entity extraction function created with official CDMX patterns\n"
     ]
    }
   ],
   "source": [
    "def extract_entities_with_official_patterns(df, text_column='text', section_column='document_section_title'):\n",
    "    \"\"\"\n",
    "    NEW REORGANIZED ENTITY EXTRACTION using official CDMX patterns\n",
    "    \n",
    "    Pattern hierarchy:\n",
    "    1. Official CDMX entities (from government registry)\n",
    "    2. Legal documents (laws, codes, regulations) \n",
    "    3. General government patterns (for non-CDMX entities)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # 1. LEGAL DOCUMENT PATTERNS\n",
    "    legal_patterns = [\n",
    "        (r'\\b(?:Ley|LEY)\\s+(?:Org√°nica|General|Federal|de|del|para|sobre|[A-Z√Å√â√ç√ì√ö√ëa-z√°√©√≠√≥√∫√±]+)\\s+[A-Z√Å√â√ç√ì√ö√ëa-z√°√©√≠√≥√∫√±\\s]{8,150}', 'LAW_MENTION'),\n",
    "        (r'\\b(?:C√≥digo|C√ìDIGO)\\s+(?:de|del|para)\\s+[A-Z√Å√â√ç√ì√ö√ëa-z√°√©√≠√≥√∫√±\\s]{10,80}', 'LAW_CODE'),\n",
    "        (r'\\b(?:Reglamento|REGLAMENTO)\\s+(?:de|del|para)\\s+[A-Z√Å√â√ç√ì√ö√ëa-z√°√©√≠√≥√∫√±\\s]{10,100}', 'REGULATION'),\n",
    "        (r'\\b(?:Norma|NORMA)\\s+Oficial\\s+Mexicana\\s+[A-Z0-9\\-]+', 'NOM'),\n",
    "        (r'\\b(?:Constituci√≥n|CONSTITUCI√ìN)(?:\\s+Pol√≠tica)?\\s+(?:de|del)\\s+[A-Z√Å√â√ç√ì√ö√ëa-z√°√©√≠√≥√∫√±\\s]{5,80}', 'CONSTITUTION'),  # Fixed to catch \"Constituci√≥n Pol√≠tica\"\n",
    "    ]\n",
    "    \n",
    "    # 2. OFFICIAL CDMX GOVERNMENT ENTITIES (from official registry)\n",
    "    official_cdmx_patterns = []\n",
    "    for pattern, full_name, category in GOV_ENTITY_REGEX:\n",
    "        # Clean the pattern - remove problematic double backslashes\n",
    "        cleaned_pattern = pattern.replace('\\\\\\\\', '\\\\')\n",
    "        official_cdmx_patterns.append((cleaned_pattern, category))\n",
    "    \n",
    "    # 3. GENERAL GOVERNMENT PATTERNS (for entities not in official CDMX registry)\n",
    "    general_gov_patterns = [\n",
    "        # Generic secretarias not captured by official patterns\n",
    "        (r'\\b(?:La\\s+|la\\s+)?(?:Secretar√≠a|SECRETAR√çA)(?:\\s+(?:de|del)\\s+[A-Z√Å√â√ç√ì√ö√ëa-z√°√©√≠√≥√∫√±\\s]{5,80})?', 'SECRETARIA_GENERAL'),\n",
    "        # Generic alcaldias not captured by official patterns  \n",
    "        (r'\\b(?:Alcald√≠a|ALCALD√çA|Alcald√≠as|ALCALD√çAS)\\b', 'ALCALDIA_GENERAL'),\n",
    "        # Federal agencies\n",
    "        (r'\\b(?:SEDEMA|SEMARNAT|CONAGUA|PROFEPA|CONANP|COFEPRIS|CONDUSEF)\\b', 'FEDERAL_AGENCY'),\n",
    "        # Autonomous federal organs\n",
    "        (r'\\b(?:INE|INAI|CNDH|COFECE|IFT|INEGI|CONEVAL)\\b', 'ORG_AUTONOMO_FED'),\n",
    "        # Federal parastatals\n",
    "        (r'\\b(?:IMSS|ISSSTE|PEMEX|CFE)\\b', 'PARAESTATAL_FED'),\n",
    "        # Generic organizational patterns\n",
    "        (r'\\b(?:Instituto|Tribunal|Consejo|Comit√©|Coordinaci√≥n|Organismo|Centro|Sistema|Registro)\\s+(?:de|para|P√∫blico)\\s+[A-Z√Å√â√ç√ì√ö√ëa-z√°√©√≠√≥√∫√±\\s]{8,120}', 'ORG_GENERICO'),\n",
    "        (r'\\b(?:Universidad|UNIVERSIDAD)\\s+[A-Z√Å√â√ç√ì√ö√ëa-z√°√©√≠√≥√∫√±\\s]{5,50}', 'UNIVERSITY'),\n",
    "    ]\n",
    "    \n",
    "    # Combine all pattern groups with metadata\n",
    "    all_pattern_groups = [\n",
    "        ('LEGAL_DOCS', legal_patterns),\n",
    "        ('CDMX_OFFICIAL', official_cdmx_patterns), \n",
    "        ('GENERAL_GOV', general_gov_patterns),\n",
    "    ]\n",
    "    \n",
    "    print(f\"Pattern groups loaded:\")\n",
    "    print(f\"  - Legal documents: {len(legal_patterns)} patterns\")\n",
    "    print(f\"  - Official CDMX entities: {len(official_cdmx_patterns)} patterns\") \n",
    "    print(f\"  - General government: {len(general_gov_patterns)} patterns\")\n",
    "    \n",
    "    counter = 1\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        text = str(row[text_column]) if pd.notna(row[text_column]) else \"\"\n",
    "        section_title = row[section_column] if pd.notna(row[section_column]) else \"\"\n",
    "        \n",
    "        # Use pre-calculated document hash (much more efficient!)\n",
    "        doc_hash = row.get('doc_hash', 'UNKNOWN')\n",
    "        \n",
    "        if not text.strip():\n",
    "            continue\n",
    "        \n",
    "        # Apply each pattern group\n",
    "        for group_name, patterns in all_pattern_groups:\n",
    "            for pattern, entity_label in patterns:\n",
    "                try:\n",
    "                    matches = re.finditer(pattern, text, re.IGNORECASE)\n",
    "                    for match in matches:\n",
    "                        results.append({\n",
    "                            'entity_id': generate_extraction_id('ENT', doc_hash, counter),\n",
    "                            'source_row_id': row.get('row_id', ''),\n",
    "                            section_column: section_title,\n",
    "                            'original_text': text,\n",
    "                            'entity_text': match.group(0).strip(),\n",
    "                            'entity_label': entity_label,\n",
    "                            'pattern_group': group_name,\n",
    "                            'start_char': match.start(),\n",
    "                            'end_char': match.end(),\n",
    "                        })\n",
    "                        counter += 1\n",
    "                except re.error as e:\n",
    "                    print(f\"Regex error in {group_name} pattern: {e}\")\n",
    "                    continue\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"NEW organized entity extraction function created with official CDMX patterns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "610e370e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting article mentions with 30-word context...\n",
      "Found 36 article mentions\n",
      "From 332 original rows\n",
      "\n",
      "Sample article mentions:\n",
      "==================================================\n",
      "\n",
      "Example 1:\n",
      "Section: Art√≠culo 1\n",
      "------------------------------\n",
      "\n",
      "Example 2:\n",
      "Section: Art√≠culo 1\n",
      "------------------------------\n",
      "\n",
      "Example 3:\n",
      "Section: Art√≠culo 25.-\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Extract article mentions with extended context\n",
    "print(\"Extracting article mentions with 30-word context...\")\n",
    "#article_mentions_df = extract_article_mentions_extended(ley_ambiental)\n",
    "article_mentions_df = extract_article_mentions_improved(ley_ambiental)\n",
    "\n",
    "\n",
    "print(f\"Found {len(article_mentions_df)} article mentions\")\n",
    "print(f\"From {len(ley_ambiental)} original rows\")\n",
    "\n",
    "# Show sample results\n",
    "if len(article_mentions_df) > 0:\n",
    "    print(\"\\nSample article mentions:\")\n",
    "    print(\"=\" * 50)\n",
    "    for i in range(min(3, len(article_mentions_df))):\n",
    "        row = article_mentions_df.iloc[i]\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"Section: {row['document_section_title']}\")\n",
    "        #print(f\"Context: {row['full_context'][:200]}...\")\n",
    "        print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "daa8a167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting entities using organized patterns with official CDMX registry...\n",
      "Pattern groups loaded:\n",
      "  - Legal documents: 5 patterns\n",
      "  - Official CDMX entities: 112 patterns\n",
      "  - General government: 7 patterns\n",
      "Found 614 entities\n",
      "\n",
      "Entity types found:\n",
      "entity_label\n",
      "SECRETARIA_GENERAL        324\n",
      "LAW_MENTION                99\n",
      "ORG_GENERICO               64\n",
      "CDMX_PODER_EJECUTIVO       49\n",
      "ALCALDIA_GENERAL           44\n",
      "CDMX_OTRO                  15\n",
      "REGULATION                  9\n",
      "CONSTITUTION                4\n",
      "CDMX_PODER_LEGISLATIVO      2\n",
      "CDMX_ORGANOS_AUTONOMOS      2\n",
      "LAW_CODE                    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample entities:\n",
      "==================================================\n",
      "\n",
      "1. LAW_MENTION\n",
      "   Text: 'Ley es reglamentaria de las disposiciones contenidas en el Apartado A del art√≠culo'\n",
      "   Section: Art√≠culo 1\n",
      "\n",
      "2. CONSTITUTION\n",
      "   Text: 'Constituci√≥n Pol√≠tica de la Ciudad de M√©xico'\n",
      "   Section: Art√≠culo 1\n",
      "\n",
      "3. ALCALDIA_GENERAL\n",
      "   Text: 'Alcald√≠as'\n",
      "   Section: Art√≠culo 1\n",
      "\n",
      "4. ORG_GENERICO\n",
      "   Text: 'organismo p√∫blico establecido para ello'\n",
      "   Section: Art√≠culo 2\n",
      "\n",
      "5. LAW_MENTION\n",
      "   Text: 'Ley de Procedimiento Administrativo de la Ciudad de M√©xico'\n",
      "   Section: Art√≠culo 3\n"
     ]
    }
   ],
   "source": [
    "# Extract entities using NEW organized patterns (includes official CDMX entities)\n",
    "print(\"Extracting entities using organized patterns with official CDMX registry...\")\n",
    "entities_df = extract_entities_with_official_patterns(ley_ambiental)\n",
    "\n",
    "print(f\"Found {len(entities_df)} entities\")\n",
    "\n",
    "if len(entities_df) > 0:\n",
    "    print(\"\\nEntity types found:\")\n",
    "    print(entities_df['entity_label'].value_counts())\n",
    "    \n",
    "    print(\"\\nSample entities:\")\n",
    "    print(\"=\" * 50)\n",
    "    for i in range(min(5, len(entities_df))):\n",
    "        row = entities_df.iloc[i]\n",
    "        print(f\"\\n{i+1}. {row['entity_label']}\")\n",
    "        print(f\"   Text: '{row['entity_text']}'\")\n",
    "        print(f\"   Section: {row['document_section_title']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "65f9811f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results...\n",
      "Article mentions saved to: /Users/alexa/Projects/cdmx_kg/Mexico_City/article_mentions_extended_context.csv\n",
      "Entities saved to: /Users/alexa/Projects/cdmx_kg/Mexico_City/entities_regex_extracted_2.csv\n",
      "\n",
      "Summary:\n",
      "- Article mentions: 36 (30-word context each)\n",
      "- Entities found: 614 (organized pattern-based extraction)\n",
      "- Both files saved with UTF-8-sig encoding for Excel compatibility\n",
      "\n",
      "Entity breakdown by pattern group:\n",
      "pattern_group\n",
      "GENERAL_GOV      432\n",
      "LEGAL_DOCS       114\n",
      "CDMX_OFFICIAL     68\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top entity types found:\n",
      "entity_label\n",
      "SECRETARIA_GENERAL        324\n",
      "LAW_MENTION                99\n",
      "ORG_GENERICO               64\n",
      "CDMX_PODER_EJECUTIVO       49\n",
      "ALCALDIA_GENERAL           44\n",
      "CDMX_OTRO                  15\n",
      "REGULATION                  9\n",
      "CONSTITUTION                4\n",
      "CDMX_PODER_LEGISLATIVO      2\n",
      "CDMX_ORGANOS_AUTONOMOS      2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CDMX Official entities breakdown:\n",
      "entity_label\n",
      "CDMX_PODER_EJECUTIVO      49\n",
      "CDMX_OTRO                 15\n",
      "CDMX_PODER_LEGISLATIVO     2\n",
      "CDMX_ORGANOS_AUTONOMOS     2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Save results with UTF-8-sig encoding for Excel compatibility\n",
    "print(\"Saving results...\")\n",
    "\n",
    "# Save original data with IDs\n",
    "#original_output = '/Users/alexa/Projects/cdmx_kg/Mexico_City/ley_ambiental_with_ids.csv'\n",
    "#ley_ambiental.to_csv(original_output, index=False, encoding='utf-8-sig')\n",
    "#print(f\"Original data with IDs saved to: {original_output}\")\n",
    "\n",
    "# Save article mentions with IDs\n",
    "articles_output = '/Users/alexa/Projects/cdmx_kg/Mexico_City/article_mentions_extended_context.csv'\n",
    "article_mentions_df.to_csv(articles_output, index=False, encoding='utf-8-sig')\n",
    "print(f\"Article mentions saved to: {articles_output}\")\n",
    "\n",
    "# Save entity recognition results with IDs\n",
    "entities_output = '/Users/alexa/Projects/cdmx_kg/Mexico_City/entities_regex_extracted_2.csv'\n",
    "entities_df.to_csv(entities_output, index=False, encoding='utf-8-sig')\n",
    "print(f\"Entities saved to: {entities_output}\")\n",
    "\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"- Article mentions: {len(article_mentions_df)} (30-word context each)\")\n",
    "print(f\"- Entities found: {len(entities_df)} (organized pattern-based extraction)\")\n",
    "print(f\"- Both files saved with UTF-8-sig encoding for Excel compatibility\")\n",
    "\n",
    "if len(entities_df) > 0:\n",
    "    print(f\"\\nEntity breakdown by pattern group:\")\n",
    "    print(entities_df['pattern_group'].value_counts())\n",
    "    \n",
    "    print(f\"\\nTop entity types found:\")\n",
    "    print(entities_df['entity_label'].value_counts().head(10))\n",
    "    \n",
    "    print(f\"\\nCDMX Official entities breakdown:\")\n",
    "    cdmx_entities = entities_df[entities_df['pattern_group'] == 'CDMX_OFFICIAL']\n",
    "    if len(cdmx_entities) > 0:\n",
    "        print(cdmx_entities['entity_label'].value_counts().head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38988c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the ID system\n",
    "print(\"ID SYSTEM DEMONSTRATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if len(ley_ambiental) > 0:\n",
    "    print(f\"Original Data IDs (sample):\")\n",
    "    print(ley_ambiental[['row_id', 'document_section_title']].head(3))\n",
    "    \n",
    "if len(article_mentions_df) > 0:\n",
    "    print(f\"\\nArticle Mention IDs (sample):\")\n",
    "    print(article_mentions_df[['article_id', 'source_row_id', 'document_section_title']].head(3))\n",
    "    \n",
    "if len(entities_df) > 0:\n",
    "    print(f\"\\nEntity IDs (sample):\")\n",
    "    print(entities_df[['entity_id', 'source_row_id', 'entity_text']].head(3))\n",
    "\n",
    "print(f\"\\nüìä ID Summary:\")\n",
    "print(f\"- Original rows: {len(ley_ambiental)} (each with unique row_id)\")\n",
    "print(f\"- Article mentions: {len(article_mentions_df)} (each with unique article_id)\")\n",
    "print(f\"- Entities: {len(entities_df)} (each with unique entity_id)\")\n",
    "print(f\"- All IDs trace back to source via document hash\")\n",
    "\n",
    "# Show ID structure\n",
    "if len(ley_ambiental) > 0:\n",
    "    sample_id = ley_ambiental['row_id'].iloc[0]\n",
    "    print(f\"\\nüîç ID Structure Example: {sample_id}\")\n",
    "    parts = sample_id.split('_')\n",
    "    if len(parts) >= 4:\n",
    "        print(f\"  - Prefix: {parts[0]} (CDMX)\")\n",
    "        print(f\"  - Document Hash: {parts[1]} (8-char hash of document name)\")\n",
    "        print(f\"  - Section: {parts[2]} (cleaned section title)\")\n",
    "        print(f\"  - Row Number: {parts[3]} (5-digit sequential)\")\n",
    "    \n",
    "print(f\"\\n‚úÖ This ID system will scale perfectly for multiple documents!\")\n",
    "print(f\"\\nüöÄ EFFICIENCY IMPROVEMENT:\")\n",
    "print(f\"   - OLD: Calculate hash {len(article_mentions_df) + len(entities_df)} times\")\n",
    "print(f\"   - NEW: Calculate hash only {len(ley_ambiental['doc_hash'].unique())} time(s)\")\n",
    "print(f\"   - Performance gain: ~{(len(article_mentions_df) + len(entities_df))//len(ley_ambiental['doc_hash'].unique())}x faster!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdmx_kg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
