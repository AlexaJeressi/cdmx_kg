{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "447421ac",
   "metadata": {},
   "source": [
    "### GPT Cleaning Pipeline\n",
    "In this code ChatGPT will clean government entities and law mentions. The objective is to:\n",
    "1. Clean government entities and extract missing ones\n",
    "2. Clean law mentions and identify self-references\n",
    "3. Create proper article + law structure\n",
    "\n",
    "The process runs in 2 phases:\n",
    "- **Phase 1**: Clean entities and laws\n",
    "- **Phase 2**: Create article-law mappings using cleaned data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd5d118",
   "metadata": {},
   "source": [
    "## üìã Processing Overview\n",
    "\n",
    "This notebook implements an automated GPT cleaning pipeline with the following objectives:\n",
    "\n",
    "### üéØ **Goals:**\n",
    "1. **Government Entities**: Clean and validate extracted entities, remove false positives, add missing ones\n",
    "2. **Law Mentions**: Clean law references, identify self-references, standardize formats  \n",
    "3. **Article-Law Structure**: Create proper mappings between articles and their corresponding laws\n",
    "\n",
    "### üîÑ **Process Flow:**\n",
    "- **Input**: Raw regex extractions from entities_extracted_complete.csv, mentions_extracted_complete.csv, identifiers_0_half.csv\n",
    "- **Phase 1**: Clean entities and laws with GPT validation\n",
    "- **Verification**: Save and review Phase 1 results  \n",
    "- **Phase 2**: Create article-law relationships using cleaned data\n",
    "- **Output**: Clean, structured CSV files ready for knowledge graph construction \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a77ae18",
   "metadata": {},
   "source": [
    "## üìä Data Sources & Structure\n",
    "\n",
    "### **Input Files:**\n",
    "1. **`entities_extracted_complete.csv`**: Government entities and laws found by regex\n",
    "   - Columns: `[row_id, entity_text, full_context, entity_label, pattern_group]`\n",
    "   \n",
    "2. **`mentions_extracted_complete.csv`**: Article mentions found by regex  \n",
    "   - Columns: `[row_id, matched_text, context_300_chars, pattern_type]`\n",
    "   \n",
    "3. **`identifiers_0_half.csv`**: Original legal document text\n",
    "   - Columns: `[row_id, document_name, document_section_title, text]`\n",
    "\n",
    "### **Output Files:**\n",
    "- **`gov_entities_cleaned.csv`**: Validated government entities with context\n",
    "- **`law_mentions_cleaned.csv`**: Cleaned law mentions with context  \n",
    "- **`article_law_mappings.csv`**: Article-to-law relationship mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0d1a7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0156565f",
   "metadata": {},
   "source": [
    "## üîß Pipeline Features\n",
    "\n",
    "### **‚ú® Key Capabilities:**\n",
    "\n",
    "#### **üèõÔ∏è Government Entity Cleaning:**\n",
    "- Remove vague references (\"las autoridades\", \"el gobierno\")\n",
    "- Keep only concrete institutional names  \n",
    "- Add missing entities not found by regex\n",
    "- Preserve original context for validation\n",
    "\n",
    "#### **‚öñÔ∏è Law Mention Processing:**\n",
    "- Clean and standardize law names\n",
    "- Identify self-references (\"esta Ley\" ‚Üí document name)\n",
    "- Handle regulations (\"su reglamento\" ‚Üí \"REGLAMENTO DE [LAW]\")\n",
    "- Remove non-official mentions\n",
    "\n",
    "#### **üìë Article-Law Mapping:**\n",
    "- Map each article to its corresponding law\n",
    "- Handle ranges (\"art√≠culos 22 y 23\" ‚Üí separate rows)\n",
    "- Distinguish current document vs external references\n",
    "- Use cleaned entities for context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f1dd14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3ed633e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load required libraries and data\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf0488ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Entities: 22316 rows\n",
      "   ‚úÖ Mentions: 3077 rows\n",
      "   ‚úÖ Identifiers: 11130 rows\n",
      "\n",
      "üìã Entities columns: ['doc_hash', 'row_id', 'section_title', 'entity_text', 'entity_label', 'pattern_group', 'before_context', 'after_context', 'full_context', 'words_before_count', 'words_after_count']\n",
      "üìã Mentions columns: ['doc_hash', 'row_id', 'pattern_type', 'document_section_title', 'matched_text', 'context_30_words', 'context_300_chars', 'start_char', 'end_char']\n",
      "üìã Identifiers columns: ['row_id', 'doc_hash', 'document_name', 'document_section_title', 'text']\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Load the three required CSV files\n",
    "\n",
    "\n",
    "# Load entities extracted\n",
    "entities_df = pd.read_csv('/Users/alexa/Projects/cdmx_kg/data/entities_extracted_complete.csv')\n",
    "print(f\"   ‚úÖ Entities: {len(entities_df)} rows\")\n",
    "\n",
    "# Load article mentions extracted\n",
    "mentions_df = pd.read_csv('/Users/alexa/Projects/cdmx_kg/data/mentions_extracted_complete.csv')\n",
    "print(f\"   ‚úÖ Mentions: {len(mentions_df)} rows\")\n",
    "\n",
    "# Load original identifiers with full text\n",
    "identifiers_df = pd.read_csv('/Users/alexa/Projects/cdmx_kg/data/identifiers_0_half.csv')\n",
    "print(f\"   ‚úÖ Identifiers: {len(identifiers_df)} rows\")\n",
    "\n",
    "print(f\"\\nüìã Entities columns: {list(entities_df.columns)}\")\n",
    "print(f\"üìã Mentions columns: {list(mentions_df.columns)}\")\n",
    "print(f\"üìã Identifiers columns: {list(identifiers_df.columns)}\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abbf824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üõ†Ô∏è STEP 2: Helper Functions\n",
    "# ===================================================\n",
    "# PURPOSE: Define utility functions needed for processing\n",
    "# FUNCTIONS: clean_csv_output() for parsing GPT responses\n",
    "# ===================================================\n",
    "\n",
    "def clean_csv_output(raw_output, expected_columns):\n",
    "    \"\"\"Clean and parse GPT's CSV response into list of lists\"\"\"\n",
    "    clean_lines = []\n",
    "    \n",
    "    for line in raw_output.strip().splitlines():\n",
    "        line = line.strip()\n",
    "        \n",
    "        # Skip empty lines\n",
    "        if not line:\n",
    "            continue\n",
    "            \n",
    "        # Skip markdown code blocks\n",
    "        if line.startswith('```'):\n",
    "            continue\n",
    "            \n",
    "        # Skip headers (case insensitive)\n",
    "        if any(header in line.lower() for header in ['row_id', 'entity_text', 'law_mention', 'article_mention']):\n",
    "            continue\n",
    "            \n",
    "        # Skip comments and instructions\n",
    "        if line.startswith('#') or line.startswith('//') or line.startswith('OUTPUT') or line.startswith('EXAMPLE'):\n",
    "            continue\n",
    "            \n",
    "        # Try to parse as CSV\n",
    "        try:\n",
    "            # Split by comma but handle commas in quoted context\n",
    "            parts = line.split(',')\n",
    "            if len(parts) >= expected_columns:\n",
    "                # Join excess parts back (for commas in context field)\n",
    "                if len(parts) > expected_columns:\n",
    "                    parts = parts[:expected_columns-1] + [','.join(parts[expected_columns-1:])]\n",
    "                clean_lines.append(parts)\n",
    "        except:\n",
    "            # Skip malformed lines\n",
    "            continue\n",
    "    \n",
    "    return clean_lines\n",
    "\n",
    "print(\"‚úÖ Helper functions defined successfully\")\n",
    "print(f\"   üìã clean_csv_output() - Parses GPT CSV responses\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11d08ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Filtering entities by type...\n",
      "üìä Total entities: 22,316\n",
      "üèõÔ∏è  Government entities: 13,948 (62.5%)\n",
      "‚öñÔ∏è  Law entities: 4,676 (21.0%)\n",
      "‚úÖ Data filtering completed!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Filter and Prepare Data\n",
    "print(\"üîç Filtering entities by type...\")\n",
    "\n",
    "# Define entity categories (government vs law entities)\n",
    "gov_entity_labels = [\n",
    "    'SECRETARIA_GENERAL', 'ALCALDIA_GENERAL', 'FEDERAL_AGENCY', 'ORG_AUTONOMO_FED', \n",
    "    'PARAESTATAL_FED', 'ORG_GENERICO', 'UNIVERSITY', 'CDMX_PODER_EJECUTIVO', \n",
    "    'CDMX_PODER_LEGISLATIVO', 'CDMX_PODER_JUDICIAL', 'CDMX_ORGANOS_AUTONOMOS',\n",
    "    'CDMX_ALCALDIAS', 'CDMX_ENTIDADES'\n",
    "]\n",
    "\n",
    "law_entity_labels = [\n",
    "    'LAW_MENTION', 'LAW_CODE', 'REGULATION', 'NOM', 'CONSTITUTION',\n",
    "    'FEDERAL_LAWS', 'CDMX_LAWS', 'LEGAL_DOCS'\n",
    "]\n",
    "\n",
    "# CREATE THE FILTERED DATAFRAMES (This was missing!)\n",
    "gov_entities = entities_df[entities_df['entity_label'].isin(gov_entity_labels)].copy()\n",
    "law_entities = entities_df[entities_df['entity_label'].isin(law_entity_labels)].copy()\n",
    "\n",
    "# Summary\n",
    "print(f\"üìä Total entities: {len(entities_df):,}\")\n",
    "print(f\"üèõÔ∏è  Government entities: {len(gov_entities):,} ({len(gov_entities)/len(entities_df)*100:.1f}%)\")\n",
    "print(f\"‚öñÔ∏è  Law entities: {len(law_entities):,} ({len(law_entities)/len(entities_df)*100:.1f}%)\")\n",
    "print(f\"‚úÖ Data filtering completed!\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81f3bf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2A: Government Entities Prompt (SIMPLIFIED)\n",
    "# Creates clean, simple prompt for government entity cleaning\n",
    "def create_prompt_gov_entities(row_id, document_name, section_title, text, gov_entities_for_row):\n",
    "    \"\"\"\n",
    "    Create prompt to clean and validate government entities for a specific article\n",
    "    \"\"\"\n",
    "    entities_text = \"\\n\".join([\n",
    "        f\"- {entity['entity_text']} (Label: {entity['entity_label']}, Context: {entity['full_context'][:100]}...)\"\n",
    "        for _, entity in gov_entities_for_row.iterrows()\n",
    "    ])\n",
    "    \n",
    "    return f\"\"\"You are an expert in Mexican government institutions and entities. Your task is to clean and validate government entities extracted from legal documents.\n",
    "\n",
    "CONTEXT:\n",
    "- Document: {document_name}\n",
    "- Section: {section_title}\n",
    "- Row ID: {row_id}\n",
    "\n",
    "ORIGINAL TEXT:\n",
    "{text}\n",
    "\n",
    "EXTRACTED GOVERNMENT ENTITIES:\n",
    "{entities_text}\n",
    "\n",
    "TASK:\n",
    "1. Check all the government entities listed above and clean their names\n",
    "2. Corroborate each entity with the context in the original text\n",
    "3. Look again at the text and verify that no government entity is missing\n",
    "4. Remove non-institutional mentions such as:\n",
    "   - Generic references: \"las autoridades\", \"el gobierno\" \n",
    "   - Descriptive phrases: \"la administraci√≥n p√∫blica\", \"los funcionarios\"\n",
    "   - Conceptual terms: \"el sector p√∫blico\", \"las instituciones\"\n",
    "   Keep only official named entities: \"Secretar√≠a de...\", \"Instituto de...\", \"Alcald√≠a...\", etc.\n",
    "5. Add any missing government entities you find in the text\n",
    "\n",
    "RULES:\n",
    "- Keep only REAL government entities (ministries, agencies, institutions, alcald√≠as, etc.)\n",
    "- Keep entity names as they appear in the context\n",
    "- Remove non-specific references and keep only concrete government institutions:\n",
    "  Remove: \"las autoridades\", \"el gobierno\", \"la administraci√≥n p√∫blica\"\n",
    "  Keep: \"SEDEMA\", \"Jefatura de Gobierno\", \"Instituto Nacional Electoral\"\n",
    "- Delete vague references that don't specify a real entity\n",
    "- Add missing entities that regex might have missed\n",
    "- Preserve entity names as they appear in the original text\n",
    "- ALWAYS provide context: Extract the exact phrase from the text where the entity appears\n",
    "- For existing regex-identified entities: Keep the original context provided\n",
    "- For new entities found: Include the surrounding text where the entity is mentioned\n",
    "- NO empty context fields - every row must have context\n",
    "\n",
    "OUTPUT FORMAT (CSV only, no headers):\n",
    "row_id,entity_text,context\n",
    "\n",
    "EXAMPLE OUTPUT:\n",
    "{row_id},SEDEMA,la Secretar√≠a del Medio Ambiente (SEDEMA)\n",
    "{row_id},Jefatura de Gobierno de la Ciudad de M√©xico,representantes de la Jefatura de Gobierno\n",
    "{row_id},Alcald√≠a Benito Ju√°rez,en coordinaci√≥n con la Alcald√≠a Benito Ju√°rez\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ecc7cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2B: Law Mentions Prompt (SIMPLIFIED)\n",
    "# Creates clean, simple prompt for law mention cleaning\n",
    "def create_prompt_law_mentions(row_id, document_name, section_title, text, law_entities_for_row):\n",
    "    \"\"\"\n",
    "    Create prompt to clean and validate law mentions for a specific article\n",
    "    \"\"\"\n",
    "    entities_text = \"\\n\".join([\n",
    "        f\"- {entity['entity_text']} (Label: {entity['entity_label']}, Context: {entity['full_context'][:100]}...)\"\n",
    "        for _, entity in law_entities_for_row.iterrows()\n",
    "    ])\n",
    "    \n",
    "    return f\"\"\"You are an expert in Mexican legal system and legislation. Your task is to clean and validate law mentions extracted from legal documents.\n",
    "\n",
    "CONTEXT:\n",
    "- Document: {document_name}\n",
    "- Section: {section_title}\n",
    "- Row ID: {row_id}\n",
    "\n",
    "ORIGINAL TEXT:\n",
    "{text}\n",
    "\n",
    "EXTRACTED LAW MENTIONS:\n",
    "{entities_text}\n",
    "\n",
    "TASK:\n",
    "1. Take all the law mentions identified by regex and clean them\n",
    "2. Delete mentions that are NOT official law mentions\n",
    "3. Identify self-reference mentions (like \"esta Ley\", \"la presente Ley\") and set the correct name: {document_name}\n",
    "4. Identify codes/regulations of a law and set the correct name: \"REGLAMENTO DE {document_name}\"\n",
    "5. Look for missing law mentions that regex might not have identified\n",
    "\n",
    "RULES:\n",
    "- Keep only REAL law mentions (Constituci√≥n, Ley, C√≥digo, Reglamento, NOM, etc.)\n",
    "- Self-references ‚Üí Use document name: {document_name}\n",
    "- Regulations ‚Üí Format as \"REGLAMENTO DE {document_name}\"\n",
    "- Complete names when only partial names are mentioned\n",
    "- Remove vague references like \"ley establecer√°\"\n",
    "- Add missing official law mentions\n",
    "- ALWAYS provide context: Extract the exact phrase from the text where the law mention appears\n",
    "- For existing regex-identified laws: Keep the original context provided\n",
    "- For self-references: Include the original phrase (e.g., \"esta Ley\", \"la presente Ley\")\n",
    "- For new laws found: Include the surrounding text where the law is mentioned\n",
    "- NO empty context fields - every row must have context\n",
    "\n",
    "OUTPUT FORMAT (CSV only, no headers):\n",
    "row_id,law_mention,context\n",
    "\n",
    "EXAMPLE OUTPUT:\n",
    "{row_id},CONSTITUCI√ìN POL√çTICA DE LOS ESTADOS UNIDOS MEXICANOS,art√≠culo 27 de la Constituci√≥n Pol√≠tica de los Estados Unidos Mexicanos\n",
    "{row_id},{document_name},conforme a lo establecido en esta Ley\n",
    "{row_id},REGLAMENTO DE {document_name},su reglamento establecer√° los procedimientos\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e77af675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2C: Article-Law Mapping Prompt (SIMPLIFIED)\n",
    "# Creates clean prompt for article-law relationship mapping\n",
    "def create_prompt_article_law_with_cleaned_data(row_id, document_name, section_title, text, article_mentions_for_row, cleaned_gov_entities, cleaned_law_mentions):\n",
    "    \"\"\"\n",
    "    Create prompt to map articles with their corresponding laws using cleaned entity and law data\n",
    "    \"\"\"\n",
    "    mentions_text = \"\\n\".join([\n",
    "        f\"- {mention['matched_text']} (Context: {mention['context_300_chars'][:150]}...)\"\n",
    "        for _, mention in article_mentions_for_row.iterrows()\n",
    "    ])\n",
    "    \n",
    "    # Include cleaned entities and laws for better context\n",
    "    cleaned_entities_text = \"\"\n",
    "    if not cleaned_gov_entities.empty:\n",
    "        cleaned_entities_text = \"\\nCLEANED GOVERNMENT ENTITIES:\\n\" + \"\\n\".join([\n",
    "            f\"- {entity['entity_text']} (Context: {entity['context'][:100]}...)\"\n",
    "            for _, entity in cleaned_gov_entities.iterrows()\n",
    "        ])\n",
    "    \n",
    "    cleaned_laws_text = \"\"\n",
    "    if not cleaned_law_mentions.empty:\n",
    "        cleaned_laws_text = \"\\nCLEANED LAW MENTIONS:\\n\" + \"\\n\".join([\n",
    "            f\"- {law['law_mention']} (Context: {law['context'][:100]}...)\"\n",
    "            for _, law in cleaned_law_mentions.iterrows()\n",
    "        ])\n",
    "    \n",
    "    return f\"\"\"You are an expert in Mexican legal citations and cross-references. Your task is to create the correct structure for article + law relationships using cleaned entity data.\n",
    "\n",
    "CONTEXT:\n",
    "- Document: {document_name}\n",
    "- Section: {section_title}\n",
    "- Row ID: {row_id}\n",
    "\n",
    "ORIGINAL TEXT:\n",
    "{text}\n",
    "\n",
    "EXTRACTED ARTICLE MENTIONS:\n",
    "{mentions_text}\n",
    "{cleaned_entities_text}\n",
    "{cleaned_laws_text}\n",
    "\n",
    "TASK:\n",
    "1. Identify all articles extracted by regex and map what law corresponds to each article\n",
    "2. Read the complete text to determine which law each article belongs to\n",
    "3. Use the cleaned law mentions above to help identify the correct law names\n",
    "4. If the article is about the current legal document, insert the current law name: {document_name}\n",
    "5. If the article is about another law, include the article number and corresponding law\n",
    "6. Create one row per article mention\n",
    "7. If there is a range of articles (e.g., \"art√≠culos 22 y 23\"), create one row per number\n",
    "8. Check the text again for missing article mentions\n",
    "\n",
    "RULES:\n",
    "- For current document references ‚Üí law_mention = {document_name}\n",
    "- For external law references ‚Üí law_mention = full law name (use cleaned law mentions when available)\n",
    "- Split ranges: \"art√≠culos 22 y 23\" = two rows (22, 23)\n",
    "- Use complete law names from the cleaned data or context\n",
    "- Article numbers should be just the number (e.g., \"22\", not \"art√≠culo 22\")\n",
    "\n",
    "OUTPUT FORMAT (CSV only, no headers):\n",
    "row_id,article_mention,law_mention\n",
    "\n",
    "EXAMPLE OUTPUT:\n",
    "{row_id},22,{document_name}\n",
    "{row_id},23,{document_name}\n",
    "{row_id},44,CONSTITUCI√ìN POL√çTICA DE LOS ESTADOS UNIDOS MEXICANOS\n",
    "{row_id},27,CONSTITUCI√ìN POL√çTICA DE LOS ESTADOS UNIDOS MEXICANOS\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d806b904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèõÔ∏è Starting GOVERNMENT ENTITIES cleaning (Phase 1A)...\n",
      "üß™ TESTING MODE: Processing first 100 articles\n",
      "üìä Articles to process: 100\n",
      "==================================================\n",
      "    ‚ùå API Error for F823AF8C_ARTCULO1: name 'clean_csv_output' is not defined\n",
      "    ‚ùå API Error for F823AF8C_ARTCULO5: name 'clean_csv_output' is not defined\n",
      "    ‚ùå API Error for F823AF8C_ARTCULO6: name 'clean_csv_output' is not defined\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 70\u001b[0m\n\u001b[1;32m     67\u001b[0m         errors_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# Reduced delay for faster processing\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚ùå Critical error for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# PHASE 1A: Government Entities Cleaning\n",
    "print(\"üèõÔ∏è Starting GOVERNMENT ENTITIES cleaning (Phase 1A)...\")\n",
    "\n",
    "# Configuration for testing/production\n",
    "BATCH_SIZE = 100  # Process first 100 articles for testing\n",
    "# BATCH_SIZE = None  # Uncomment for full processing\n",
    "\n",
    "# Get articles to process\n",
    "if BATCH_SIZE:\n",
    "    unique_row_ids = identifiers_df['row_id'].unique()[:BATCH_SIZE]\n",
    "    print(f\"üß™ TESTING MODE: Processing first {BATCH_SIZE} articles\")\n",
    "else:\n",
    "    unique_row_ids = identifiers_df['row_id'].unique()\n",
    "    print(f\"üè≠ PRODUCTION MODE: Processing all {len(unique_row_ids)} articles\")\n",
    "\n",
    "# Initialize collectors\n",
    "gov_entities_cleaned = []\n",
    "processed_count = 0\n",
    "errors_count = 0\n",
    "skipped_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"üìä Articles to process: {len(unique_row_ids)}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, row_id in enumerate(unique_row_ids):\n",
    "    processed_count += 1\n",
    "    \n",
    "    # Progress indicator\n",
    "    if processed_count % 10 == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        rate = processed_count / elapsed if elapsed > 0 else 0\n",
    "        eta = (len(unique_row_ids) - processed_count) / rate if rate > 0 else 0\n",
    "        print(f\"üìà Progress: {processed_count}/{len(unique_row_ids)} | Rate: {rate:.1f}/s | ETA: {eta:.0f}s | Errors: {errors_count}\")\n",
    "    \n",
    "    try:\n",
    "        # Get article info\n",
    "        article_info = identifiers_df[identifiers_df['row_id'] == row_id].iloc[0]\n",
    "        document_name = article_info['document_name']\n",
    "        section_title = article_info['document_section_title']\n",
    "        text = article_info['text']\n",
    "        \n",
    "        # Get government entities for this article (with deduplication)\n",
    "        gov_entities_raw = gov_entities[gov_entities['row_id'] == row_id]\n",
    "        \n",
    "        if len(gov_entities_raw) == 0:\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "            \n",
    "        # Deduplicate\n",
    "        gov_entities_unique = gov_entities_raw.drop_duplicates(subset=['entity_text'], keep='first')\n",
    "        \n",
    "        # Process with GPT\n",
    "        try:\n",
    "            prompt = create_prompt_gov_entities(row_id, document_name, section_title, text, gov_entities_unique)\n",
    "            resp = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.1\n",
    "            )\n",
    "            output = resp.choices[0].message.content.strip()\n",
    "            cleaned_results = clean_csv_output(output, 3)\n",
    "            gov_entities_cleaned.extend(cleaned_results)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ùå API Error for {row_id}: {e}\")\n",
    "            errors_count += 1\n",
    "        \n",
    "        # Reduced delay for faster processing\n",
    "        time.sleep(0.1)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Critical error for {row_id}: {e}\")\n",
    "        errors_count += 1\n",
    "\n",
    "# Results\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"\\n‚úÖ GOVERNMENT ENTITIES CLEANING COMPLETED!\")\n",
    "print(f\"‚è±Ô∏è  Total time: {elapsed_time:.1f} seconds\")\n",
    "print(f\"üìä Processed: {processed_count} articles\")\n",
    "print(f\"‚è≠Ô∏è  Skipped (no entities): {skipped_count}\")\n",
    "print(f\"‚ùå Errors: {errors_count}\")\n",
    "print(f\"üèõÔ∏è  Gov entities cleaned: {len(gov_entities_cleaned)}\")\n",
    "print(f\"‚ö° Rate: {processed_count/elapsed_time:.1f} articles/second\")\n",
    "\n",
    "# Create DataFrame\n",
    "gov_entities_cleaned_df = pd.DataFrame(gov_entities_cleaned, columns=[\"row_id\", \"entity_text\", \"context\"]) if gov_entities_cleaned else pd.DataFrame()\n",
    "print(f\"üìã Created DataFrame with {len(gov_entities_cleaned_df)} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41748f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ PHASE 1B: Law Mentions Cleaning\n",
    "print(\"‚öñÔ∏è Starting LAW MENTIONS cleaning (Phase 1B)...\")\n",
    "\n",
    "# Use same configuration as Phase 1A\n",
    "if 'BATCH_SIZE' not in locals():\n",
    "    BATCH_SIZE = 100\n",
    "\n",
    "# Get articles to process (same as government entities)\n",
    "if BATCH_SIZE:\n",
    "    unique_row_ids = identifiers_df['row_id'].unique()[:BATCH_SIZE]\n",
    "    print(f\"üß™ TESTING MODE: Processing first {BATCH_SIZE} articles\")\n",
    "else:\n",
    "    unique_row_ids = identifiers_df['row_id'].unique()\n",
    "    print(f\"üè≠ PRODUCTION MODE: Processing all {len(unique_row_ids)} articles\")\n",
    "\n",
    "# Initialize collectors\n",
    "law_mentions_cleaned = []\n",
    "processed_count = 0\n",
    "errors_count = 0\n",
    "skipped_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"üìä Articles to process: {len(unique_row_ids)}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, row_id in enumerate(unique_row_ids):\n",
    "    processed_count += 1\n",
    "    \n",
    "    # Progress indicator\n",
    "    if processed_count % 10 == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        rate = processed_count / elapsed if elapsed > 0 else 0\n",
    "        eta = (len(unique_row_ids) - processed_count) / rate if rate > 0 else 0\n",
    "        print(f\"üìà Progress: {processed_count}/{len(unique_row_ids)} | Rate: {rate:.1f}/s | ETA: {eta:.0f}s\")\n",
    "    \n",
    "    try:\n",
    "        # Get article info\n",
    "        article_info = identifiers_df[identifiers_df['row_id'] == row_id].iloc[0]\n",
    "        document_name = article_info['document_name']\n",
    "        section_title = article_info['document_section_title']\n",
    "        text = article_info['text']\n",
    "        \n",
    "        # Get law entities for this article\n",
    "        law_entities_raw = law_entities[law_entities['row_id'] == row_id]\n",
    "        \n",
    "        if len(law_entities_raw) == 0:\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "            \n",
    "        # Deduplicate\n",
    "        law_entities_unique = law_entities_raw.drop_duplicates(subset=['entity_text'], keep='first')\n",
    "        \n",
    "        # Process with GPT\n",
    "        try:\n",
    "            prompt = create_prompt_law_mentions(row_id, document_name, section_title, text, law_entities_unique)\n",
    "            resp = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.1\n",
    "            )\n",
    "            output = resp.choices[0].message.content.strip()\n",
    "            cleaned_results = clean_csv_output(output, 3)\n",
    "            law_mentions_cleaned.extend(cleaned_results)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ùå API Error for {row_id}: {e}\")\n",
    "            errors_count += 1\n",
    "        \n",
    "        # Reduced delay\n",
    "        time.sleep(0.1)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Critical error for {row_id}: {e}\")\n",
    "        errors_count += 1\n",
    "\n",
    "# Results\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"\\n‚úÖ LAW MENTIONS CLEANING COMPLETED!\")\n",
    "print(f\"‚è±Ô∏è  Total time: {elapsed_time:.1f} seconds\")\n",
    "print(f\"üìä Processed: {processed_count} articles\")\n",
    "print(f\"‚è≠Ô∏è  Skipped (no entities): {skipped_count}\")\n",
    "print(f\"‚ùå Errors: {errors_count}\")\n",
    "print(f\"‚öñÔ∏è  Law mentions cleaned: {len(law_mentions_cleaned)}\")\n",
    "print(f\"‚ö° Rate: {processed_count/elapsed_time:.1f} articles/second\")\n",
    "\n",
    "# Create DataFrame\n",
    "law_mentions_cleaned_df = pd.DataFrame(law_mentions_cleaned, columns=[\"row_id\", \"law_mention\", \"context\"]) if law_mentions_cleaned else pd.DataFrame()\n",
    "print(f\"üìã Created DataFrame with {len(law_mentions_cleaned_df)} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7028e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä COMBINE & SAVE Phase 1 Results\n",
    "print(\"üìã Combining Phase 1A and 1B results...\")\n",
    "\n",
    "# Ensure DataFrames exist\n",
    "if 'gov_entities_cleaned_df' not in locals():\n",
    "    gov_entities_cleaned_df = pd.DataFrame(columns=[\"row_id\", \"entity_text\", \"context\"])\n",
    "    print(\"‚ö†Ô∏è  No government entities DataFrame found\")\n",
    "\n",
    "if 'law_mentions_cleaned_df' not in locals():\n",
    "    law_mentions_cleaned_df = pd.DataFrame(columns=[\"row_id\", \"law_mention\", \"context\"])\n",
    "    print(\"‚ö†Ô∏è  No law mentions DataFrame found\")\n",
    "\n",
    "# Save results\n",
    "if not gov_entities_cleaned_df.empty:\n",
    "    gov_output = '/Users/alexa/Projects/cdmx_kg/data/gov_entities_phase1_cleaned.csv'\n",
    "    gov_entities_cleaned_df.to_csv(gov_output, index=False, encoding='utf-8-sig')\n",
    "    print(f\"‚úÖ Government entities saved: {gov_output}\")\n",
    "    print(f\"   üìä {len(gov_entities_cleaned_df)} rows, {gov_entities_cleaned_df['entity_text'].nunique()} unique entities\")\n",
    "\n",
    "if not law_mentions_cleaned_df.empty:\n",
    "    law_output = '/Users/alexa/Projects/cdmx_kg/data/law_mentions_phase1_cleaned.csv'\n",
    "    law_mentions_cleaned_df.to_csv(law_output, index=False, encoding='utf-8-sig')\n",
    "    print(f\"‚úÖ Law mentions saved: {law_output}\")\n",
    "    print(f\"   üìä {len(law_mentions_cleaned_df)} rows, {law_mentions_cleaned_df['law_mention'].nunique()} unique laws\")\n",
    "\n",
    "# Create combined summary\n",
    "total_entities = len(gov_entities_cleaned_df) + len(law_mentions_cleaned_df)\n",
    "print(f\"\\nüéØ PHASE 1 SUMMARY:\")\n",
    "print(f\"   üèõÔ∏è  Government entities: {len(gov_entities_cleaned_df)}\")\n",
    "print(f\"   ‚öñÔ∏è  Law mentions: {len(law_mentions_cleaned_df)}\")\n",
    "print(f\"   üìä Total cleaned entities: {total_entities}\")\n",
    "\n",
    "if total_entities > 0:\n",
    "    print(f\"\\n‚úÖ Ready for Phase 2: Article-Law mapping!\")\n",
    "    print(f\"üìÅ Input files created for Phase 2 verification\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  No entities found - check your data or increase BATCH_SIZE\")\n",
    "\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "812666f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduplication helper function loaded\n"
     ]
    }
   ],
   "source": [
    "# Helper function for deduplication\n",
    "def deduplicate_entities_per_article(gov_entities_raw, law_entities_raw, row_id, processed_count):\n",
    "    \"\"\"\n",
    "    Deduplicate entities per article to avoid redundant processing\n",
    "    \"\"\"\n",
    "    # DEDUPLICATION: Keep only unique entities per article to avoid redundant processing\n",
    "    gov_entities_unique = gov_entities_raw.drop_duplicates(subset=['entity_text'], keep='first')\n",
    "    law_entities_unique = law_entities_raw.drop_duplicates(subset=['entity_text'], keep='first')\n",
    "    \n",
    "    # Log deduplication results (show occasionally to avoid spam)\n",
    "    if len(gov_entities_raw) != len(gov_entities_unique) and processed_count % 50 == 0:\n",
    "        removed_gov = len(gov_entities_raw) - len(gov_entities_unique)\n",
    "        print(f\"    üîÑ Deduplicated {removed_gov} duplicate gov entities for {row_id}\")\n",
    "    \n",
    "    if len(law_entities_raw) != len(law_entities_unique) and processed_count % 50 == 0:\n",
    "        removed_law = len(law_entities_raw) - len(law_entities_unique)\n",
    "        print(f\"    üîÑ Deduplicated {removed_law} duplicate law entities for {row_id}\")\n",
    "    \n",
    "    return gov_entities_unique, law_entities_unique\n",
    "\n",
    "print(\"Deduplication helper function loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "738010b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROMPT 3: Article + law structure creation\n",
    "def create_prompt_article_law(row_id, document_name, section_title, text, article_mentions_for_row):\n",
    "    \"\"\"\n",
    "    Create prompt to map articles with their corresponding laws\n",
    "    \"\"\"\n",
    "    mentions_text = \"\\n\".join([\n",
    "        f\"- {mention['matched_text']} (Context: {mention['context_300_chars'][:150]}...)\"\n",
    "        for _, mention in article_mentions_for_row.iterrows()\n",
    "    ])\n",
    "    \n",
    "    return f\"\"\"You are an expert in Mexican legal citations and cross-references. Your task is to create the correct structure for article + law relationships.\n",
    "\n",
    "CONTEXT:\n",
    "- Document: {document_name}\n",
    "- Section: {section_title}\n",
    "- Row ID: {row_id}\n",
    "\n",
    "ORIGINAL TEXT:\n",
    "{text}\n",
    "\n",
    "EXTRACTED ARTICLE MENTIONS:\n",
    "{mentions_text}\n",
    "\n",
    "TASK:\n",
    "1. Identify all articles extracted by regex and map what law corresponds to each article\n",
    "2. Read the complete text to determine which law each article belongs to\n",
    "3. If the article is about the current legal document, insert the current law name\n",
    "4. If the article is about another law, include the article number and corresponding law\n",
    "5. Create one row per article mention\n",
    "6. If there is a range of articles (e.g., \"art√≠culos 22 y 23\"), create one row per number\n",
    "7. Check the text again for missing article mentions\n",
    "\n",
    "RULES:\n",
    "- For current document references ‚Üí law_mention = current law name\n",
    "- For external law references ‚Üí law_mention = full law name\n",
    "- Split ranges: \"art√≠culos 22 y 23\" = two rows (22, 23)\n",
    "- Use complete law names from the context\n",
    "- Article numbers should be just the number (e.g., \"22\", not \"art√≠culo 22\")\n",
    "\n",
    "OUTPUT FORMAT (CSV only, no headers):\n",
    "row_id,article_mention,law_mention\n",
    "\n",
    "EXAMPLE OUTPUT:\n",
    "{row_id},22,{document_name}\n",
    "{row_id},23,{document_name}\n",
    "{row_id},44,CONSTITUCI√ìN POL√çTICA DE LOS ESTADOS UNIDOS MEXICANOS\n",
    "{row_id},27,CONSTITUCI√ìN POL√çTICA DE LOS ESTADOS UNIDOS MEXICANOS\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "738d5346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to clean and validate GPT output\n",
    "def clean_csv_output(raw_output, expected_columns):\n",
    "    \"\"\"Clean and validate CSV output from GPT, filtering out artifacts and malformed entries\"\"\"\n",
    "    clean_lines = []\n",
    "    for line in raw_output.strip().splitlines():\n",
    "        line = line.strip()\n",
    "        \n",
    "        # Skip empty lines\n",
    "        if not line:\n",
    "            continue\n",
    "            \n",
    "        # Skip markdown code blocks\n",
    "        if line.startswith('```'):\n",
    "            continue\n",
    "            \n",
    "        # Skip headers (case insensitive)\n",
    "        if any(header in line.lower() for header in ['row_id', 'entity_text', 'law_mention', 'article_mention']):\n",
    "            continue\n",
    "            \n",
    "        # Skip lines that look like instructions or comments\n",
    "        if line.startswith('#') or line.startswith('//') or line.startswith('OUTPUT') or line.startswith('EXAMPLE'):\n",
    "            continue\n",
    "            \n",
    "        # Must contain commas for CSV format\n",
    "        if ',' not in line:\n",
    "            continue\n",
    "            \n",
    "        # Check if line has the expected number of columns (allow some flexibility)\n",
    "        parts = line.split(',')\n",
    "        if len(parts) < expected_columns:\n",
    "            print(f\"    Skipping malformed line (too few columns): {line[:50]}...\")\n",
    "            continue\n",
    "            \n",
    "        # Clean each part\n",
    "        cleaned_parts = []\n",
    "        for i, part in enumerate(parts[:expected_columns]):  # Only take expected number of columns\n",
    "            cleaned_part = part.strip().strip('\"').strip(\"'\")  # Remove quotes and extra spaces\n",
    "            cleaned_parts.append(cleaned_part)\n",
    "            \n",
    "        # Skip if essential fields are empty (first column should not be empty)\n",
    "        if not cleaned_parts[0]:\n",
    "            print(f\"    Skipping line with empty row_id: {line[:50]}...\")\n",
    "            continue\n",
    "            \n",
    "        clean_lines.append(cleaned_parts)\n",
    "    \n",
    "    return clean_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7dffc588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAFETY CHECK: Create DataFrames if they don't exist\n",
    "if 'gov_entities_cleaned_df' not in locals():\n",
    "    print(\"‚ö†Ô∏è  Creating DataFrames from Phase 1 results...\")\n",
    "    if 'gov_entities_cleaned' in locals() and gov_entities_cleaned:\n",
    "        gov_entities_cleaned_df = pd.DataFrame(gov_entities_cleaned, columns=[\"row_id\", \"entity_text\", \"context\"])\n",
    "    else:\n",
    "        gov_entities_cleaned_df = pd.DataFrame()\n",
    "        \n",
    "    if 'law_mentions_cleaned' in locals() and law_mentions_cleaned:\n",
    "        law_mentions_cleaned_df = pd.DataFrame(law_mentions_cleaned, columns=[\"row_id\", \"law_mention\", \"context\"])\n",
    "    else:\n",
    "        law_mentions_cleaned_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "261556d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving Phase 1 results for verification...\n",
      "   ‚ö†Ô∏è  No government entities to save from Phase 1\n",
      " No law mentions to save from Phase 1\n"
     ]
    }
   ],
   "source": [
    "# Save intermediate results from Phase 1 for verification\n",
    "print(\"üíæ Saving Phase 1 results for verification...\")\n",
    "\n",
    "# Save government entities cleaned (Phase 1)\n",
    "if not gov_entities_cleaned_df.empty:\n",
    "    gov_entities_phase1_output = '/Users/alexa/Projects/cdmx_kg/data/gov_entities_phase1_cleaned.csv'\n",
    "    gov_entities_cleaned_df.to_csv(gov_entities_phase1_output, index=False, encoding='utf-8-sig')\n",
    "    print(f\"   ‚úÖ Phase 1 Government entities saved to: {gov_entities_phase1_output}\")\n",
    "    print(f\"      üìä Total entities: {len(gov_entities_cleaned_df)}\")\n",
    "    print(f\"      üìä Unique entities: {gov_entities_cleaned_df['entity_text'].nunique()}\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(\"Sample government entities:\")\n",
    "    sample_gov = gov_entities_cleaned_df.head(3)\n",
    "    for _, row in sample_gov.iterrows():\n",
    "        print(f\"{row['row_id']}: {row['entity_text']} | Context: {row['context'][:50]}...\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  No government entities to save from Phase 1\")\n",
    "\n",
    "# Save law mentions cleaned (Phase 1) \n",
    "if not law_mentions_cleaned_df.empty:\n",
    "    law_mentions_phase1_output = '/Users/alexa/Projects/cdmx_kg/data/law_mentions_phase1_cleaned.csv'\n",
    "    law_mentions_cleaned_df.to_csv(law_mentions_phase1_output, index=False, encoding='utf-8-sig')\n",
    "    print(f\" Phase 1 Law mentions saved to: {law_mentions_phase1_output}\")\n",
    "    print(f\" Total mentions: {len(law_mentions_cleaned_df)}\")\n",
    "    print(f\" Unique laws: {law_mentions_cleaned_df['law_mention'].nunique()}\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(\"Sample law mentions:\")\n",
    "    sample_law = law_mentions_cleaned_df.head(3)\n",
    "    for _, row in sample_law.iterrows():\n",
    "        print(f\"{row['row_id']}: {row['law_mention']} | Context: {row['context'][:50]}...\")\n",
    "else:\n",
    "    print(\" No law mentions to save from Phase 1\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0afe1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting GPT processing - PHASE 1: Entity and Law Cleaning...\n",
      "üìä Total articles to process: 10726\n",
      "üìà Phase 1 Progress: 10/10726 (0.1%) | Errors: 0\n",
      "üìà Phase 1 Progress: 20/10726 (0.2%) | Errors: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     prompt1 \u001b[38;5;241m=\u001b[39m create_prompt_gov_entities(row_id, document_name, section_title, text, gov_entities_for_row)\n\u001b[0;32m---> 48\u001b[0m     resp1 \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4o-mini\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt1\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     output1 \u001b[38;5;241m=\u001b[39m resp1\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     54\u001b[0m     cleaned_gov \u001b[38;5;241m=\u001b[39m clean_csv_output(output1, \u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cdmx_kg/lib/python3.10/site-packages/openai/_utils/_utils.py:286\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cdmx_kg/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:1147\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m   1103\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m   1145\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m   1146\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m-> 1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt_cache_key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msafety_identifier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mverbosity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweb_search_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cdmx_kg/lib/python3.10/site-packages/openai/_base_client.py:1259\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1246\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1247\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1254\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1255\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1256\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1257\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1258\u001b[0m     )\n\u001b[0;32m-> 1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cdmx_kg/lib/python3.10/site-packages/openai/_base_client.py:982\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m    980\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 982\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    988\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cdmx_kg/lib/python3.10/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cdmx_kg/lib/python3.10/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cdmx_kg/lib/python3.10/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cdmx_kg/lib/python3.10/site-packages/httpx/_client.py:1014\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1010\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1011\u001b[0m     )\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1014\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1018\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cdmx_kg/lib/python3.10/site-packages/httpx/_transports/default.py:250\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    237\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    238\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    239\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    247\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    248\u001b[0m )\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 250\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    255\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    256\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    257\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    258\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    259\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cdmx_kg/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    253\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cdmx_kg/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    232\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cdmx_kg/lib/python3.10/site-packages/httpcore/_sync/connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cdmx_kg/lib/python3.10/site-packages/httpcore/_sync/http11.py:136\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cdmx_kg/lib/python3.10/site-packages/httpcore/_sync/http11.py:106\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m     99\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    100\u001b[0m     (\n\u001b[1;32m    101\u001b[0m         http_version,\n\u001b[1;32m    102\u001b[0m         status,\n\u001b[1;32m    103\u001b[0m         reason_phrase,\n\u001b[1;32m    104\u001b[0m         headers,\n\u001b[1;32m    105\u001b[0m         trailing_data,\n\u001b[0;32m--> 106\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    114\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cdmx_kg/lib/python3.10/site-packages/httpcore/_sync/http11.py:177\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    174\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cdmx_kg/lib/python3.10/site-packages/httpcore/_sync/http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cdmx_kg/lib/python3.10/site-packages/httpcore/_backends/sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cdmx_kg/lib/python3.10/ssl.py:1292\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1288\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1289\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1290\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1291\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1292\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cdmx_kg/lib/python3.10/ssl.py:1165\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# PHASE 1: Entity and Law Cleaning\n",
    "print(\"üöÄ Starting GPT processing - PHASE 1: Entity and Law Cleaning...\")\n",
    "\n",
    "# Initialize result collectors for Phase 1\n",
    "gov_entities_cleaned = []\n",
    "law_mentions_cleaned = []\n",
    "\n",
    "# Get unique row_ids from identifiers_df to process\n",
    "unique_row_ids = identifiers_df['row_id'].unique()\n",
    "total_articles = len(unique_row_ids)\n",
    "print(f\"üìä Total articles to process: {total_articles}\")\n",
    "\n",
    "# Processing configuration\n",
    "processed_count = 0\n",
    "errors_count = 0\n",
    "\n",
    "for i, row_id in enumerate(unique_row_ids):\n",
    "    processed_count += 1\n",
    "    \n",
    "    # Progress indicator\n",
    "    if processed_count % 10 == 0 or processed_count == total_articles:\n",
    "        progress = (processed_count / total_articles) * 100\n",
    "        print(f\"üìà Phase 1 Progress: {processed_count}/{total_articles} ({progress:.1f}%) | Errors: {errors_count}\")\n",
    "    \n",
    "    try:\n",
    "        # Get article information\n",
    "        article_info = identifiers_df[identifiers_df['row_id'] == row_id].iloc[0]\n",
    "        document_name = article_info['document_name']\n",
    "        section_title = article_info['document_section_title']\n",
    "        text = article_info['text']\n",
    "        \n",
    "        # Get related entities and mentions for this row_id\n",
    "        #gov_entities_for_row = gov_entities[gov_entities['row_id'] == row_id]\n",
    "        #law_entities_for_row = law_entities[law_entities['row_id'] == row_id]\n",
    "\n",
    "        gov_entities_for_row_raw = gov_entities[gov_entities['row_id'] == row_id]\n",
    "        law_entities_for_row_raw = law_entities[law_entities['row_id'] == row_id]\n",
    "        gov_entities_for_row, law_entities_for_row = deduplicate_entities_per_article(\n",
    "        gov_entities_for_row_raw, law_entities_for_row_raw, row_id, processed_count\n",
    "         )\n",
    "        \n",
    "\n",
    "\n",
    "        # PROCESS 1: Government entities cleaning (only if entities exist)\n",
    "        if len(gov_entities_for_row) > 0:\n",
    "            try:\n",
    "                prompt1 = create_prompt_gov_entities(row_id, document_name, section_title, text, gov_entities_for_row)\n",
    "                resp1 = client.chat.completions.create(\n",
    "                    model=\"gpt-4o-mini\",\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt1}],\n",
    "                    temperature=0.1\n",
    "                )\n",
    "                output1 = resp1.choices[0].message.content.strip()\n",
    "                cleaned_gov = clean_csv_output(output1, 3)\n",
    "                gov_entities_cleaned.extend(cleaned_gov)\n",
    "            except Exception as e:\n",
    "                print(f\"    ‚ùå Error processing gov entities for {row_id}: {e}\")\n",
    "                errors_count += 1\n",
    "        \n",
    "        # PROCESS 2: Law mentions cleaning (only if entities exist)\n",
    "        if len(law_entities_for_row) > 0:\n",
    "            try:\n",
    "                prompt2 = create_prompt_law_mentions(row_id, document_name, section_title, text, law_entities_for_row)\n",
    "                resp2 = client.chat.completions.create(\n",
    "                    model=\"gpt-4o-mini\",\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt2}],\n",
    "                    temperature=0.1\n",
    "                )\n",
    "                output2 = resp2.choices[0].message.content.strip()\n",
    "                cleaned_laws = clean_csv_output(output2, 3)\n",
    "                law_mentions_cleaned.extend(cleaned_laws)\n",
    "            except Exception as e:\n",
    "                print(f\"    ‚ùå Error processing law mentions for {row_id}: {e}\")\n",
    "                errors_count += 1\n",
    "        \n",
    "        # Small delay to avoid rate limits\n",
    "        time.sleep(0.2)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Critical error processing {row_id}: {e}\")\n",
    "        errors_count += 1\n",
    "        continue\n",
    "\n",
    "print(f\"\\n‚úÖ PHASE 1 completed!\")\n",
    "print(f\"üìä Total articles processed: {processed_count}\")\n",
    "print(f\"‚ùå Total errors: {errors_count}\")\n",
    "print(f\"üèõÔ∏è  Government entities cleaned: {len(gov_entities_cleaned)}\")\n",
    "print(f\"‚öñÔ∏è  Law mentions cleaned: {len(law_mentions_cleaned)}\")\n",
    "\n",
    "# Convert to DataFrames for Phase 2\n",
    "gov_entities_cleaned_df = pd.DataFrame(gov_entities_cleaned, columns=[\"row_id\", \"entity_text\", \"context\"]) if gov_entities_cleaned else pd.DataFrame()\n",
    "law_mentions_cleaned_df = pd.DataFrame(law_mentions_cleaned, columns=[\"row_id\", \"law_mention\", \"context\"]) if law_mentions_cleaned else pd.DataFrame()\n",
    "\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc81791d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHASE 2: Article + Law Structure Creation using cleaned results\n",
    "print(\"üöÄ Starting GPT processing - PHASE 2: Article + Law Structure Creation...\")\n",
    "\n",
    "# Check if Phase 1 DataFrames exist, if not create empty ones\n",
    "if 'gov_entities_cleaned_df' not in locals():\n",
    "    print(\"‚ö†Ô∏è  Phase 1 DataFrames not found. Creating empty DataFrames...\")\n",
    "    gov_entities_cleaned_df = pd.DataFrame()\n",
    "    law_mentions_cleaned_df = pd.DataFrame()\n",
    "\n",
    "# Initialize result collectors for Phase 2\n",
    "article_law_mappings = []\n",
    "\n",
    "# Reset counters\n",
    "processed_count = 0\n",
    "errors_count = 0\n",
    "\n",
    "for i, row_id in enumerate(unique_row_ids):\n",
    "    processed_count += 1\n",
    "    \n",
    "    # Progress indicator\n",
    "    if processed_count % 10 == 0 or processed_count == total_articles:\n",
    "        progress = (processed_count / total_articles) * 100\n",
    "        print(f\"üìà Phase 2 Progress: {processed_count}/{total_articles} ({progress:.1f}%) | Errors: {errors_count}\")\n",
    "    \n",
    "    try:\n",
    "        # Get article information\n",
    "        article_info = identifiers_df[identifiers_df['row_id'] == row_id].iloc[0]\n",
    "        document_name = article_info['document_name']\n",
    "        section_title = article_info['document_section_title']\n",
    "        text = article_info['text']\n",
    "        \n",
    "        # Get article mentions for this row_id\n",
    "        article_mentions_for_row = mentions_df[mentions_df['row_id'] == row_id]\n",
    "        \n",
    "        # Get cleaned entities and laws for this row_id (from Phase 1 results)\n",
    "        cleaned_gov_for_row = gov_entities_cleaned_df[gov_entities_cleaned_df['row_id'] == row_id] if not gov_entities_cleaned_df.empty else pd.DataFrame()\n",
    "        cleaned_laws_for_row = law_mentions_cleaned_df[law_mentions_cleaned_df['row_id'] == row_id] if not law_mentions_cleaned_df.empty else pd.DataFrame()\n",
    "        \n",
    "        # PROCESS 3: Article + law structure (only if mentions exist)\n",
    "        if len(article_mentions_for_row) > 0:\n",
    "            try:\n",
    "                # Update the prompt to include cleaned results\n",
    "                prompt3 = create_prompt_article_law_with_cleaned_data(\n",
    "                    row_id, document_name, section_title, text, \n",
    "                    article_mentions_for_row, cleaned_gov_for_row, cleaned_laws_for_row\n",
    "                )\n",
    "                resp3 = client.chat.completions.create(\n",
    "                    model=\"gpt-4o-mini\",\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt3}],\n",
    "                    temperature=0.1\n",
    "                )\n",
    "                output3 = resp3.choices[0].message.content.strip()\n",
    "                cleaned_articles = clean_csv_output(output3, 3)\n",
    "                article_law_mappings.extend(cleaned_articles)\n",
    "            except Exception as e:\n",
    "                print(f\"    ‚ùå Error processing article mappings for {row_id}: {e}\")\n",
    "                errors_count += 1\n",
    "        \n",
    "        # Small delay to avoid rate limits\n",
    "        time.sleep(0.2)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Critical error processing {row_id}: {e}\")\n",
    "        errors_count += 1\n",
    "        continue\n",
    "\n",
    "print(f\"\\n‚úÖ PHASE 2 completed!\")\n",
    "print(f\"üìä Total articles processed: {processed_count}\")\n",
    "print(f\"‚ùå Total errors: {errors_count}\")\n",
    "print(f\"üìë Article-law mappings: {len(article_law_mappings)}\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df142aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ OPTIMIZED SEPARATE PROCESSING: Government Entities Only\n",
    "print(\"üèõÔ∏è Starting GOVERNMENT ENTITIES cleaning (Phase 1A)...\")\n",
    "\n",
    "# Configuration for testing/production\n",
    "BATCH_SIZE = 100  # Process first 100 articles for testing\n",
    "# BATCH_SIZE = None  # Uncomment for full processing\n",
    "\n",
    "# Get articles to process\n",
    "if BATCH_SIZE:\n",
    "    unique_row_ids = identifiers_df['row_id'].unique()[:BATCH_SIZE]\n",
    "    print(f\"üß™ TESTING MODE: Processing first {BATCH_SIZE} articles\")\n",
    "else:\n",
    "    unique_row_ids = identifiers_df['row_id'].unique()\n",
    "    print(f\"üè≠ PRODUCTION MODE: Processing all {len(unique_row_ids)} articles\")\n",
    "\n",
    "# Initialize collectors\n",
    "gov_entities_cleaned = []\n",
    "processed_count = 0\n",
    "errors_count = 0\n",
    "skipped_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"üìä Articles to process: {len(unique_row_ids)}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, row_id in enumerate(unique_row_ids):\n",
    "    processed_count += 1\n",
    "    \n",
    "    # Progress indicator\n",
    "    if processed_count % 10 == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        rate = processed_count / elapsed if elapsed > 0 else 0\n",
    "        eta = (len(unique_row_ids) - processed_count) / rate if rate > 0 else 0\n",
    "        print(f\"üìà Progress: {processed_count}/{len(unique_row_ids)} | Rate: {rate:.1f}/s | ETA: {eta:.0f}s | Errors: {errors_count}\")\n",
    "    \n",
    "    try:\n",
    "        # Get article info\n",
    "        article_info = identifiers_df[identifiers_df['row_id'] == row_id].iloc[0]\n",
    "        document_name = article_info['document_name']\n",
    "        section_title = article_info['document_section_title']\n",
    "        text = article_info['text']\n",
    "        \n",
    "        # Get government entities for this article (with deduplication)\n",
    "        gov_entities_raw = gov_entities[gov_entities['row_id'] == row_id]\n",
    "        \n",
    "        if len(gov_entities_raw) == 0:\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "            \n",
    "        # Deduplicate\n",
    "        gov_entities_unique = gov_entities_raw.drop_duplicates(subset=['entity_text'], keep='first')\n",
    "        \n",
    "                         # Process with GPT\n",
    "        try:\n",
    "             prompt = create_prompt_gov_entities(row_id, document_name, section_title, text, gov_entities_unique)\n",
    "             resp = client.chat.completions.create(\n",
    "                 model=\"gpt-4o-mini\",\n",
    "                 messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                 temperature=0.1\n",
    "             )\n",
    "             output = resp.choices[0].message.content.strip()\n",
    "             cleaned_results = clean_csv_output(output, 3)\n",
    "             gov_entities_cleaned.extend(cleaned_results)\n",
    "             \n",
    "         except Exception as e:\n",
    "             print(f\"    ‚ùå API Error for {row_id}: {e}\")\n",
    "             errors_count += 1\n",
    "        \n",
    "        # Reduced delay for faster processing\n",
    "        time.sleep(0.1)  # Reduced from 0.2s\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\\\"‚ùå Critical error for {row_id}: {e}\\\")\n",
    "        errors_count += 1\n",
    "\n",
    "# Results\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\\\"\\\\n‚úÖ GOVERNMENT ENTITIES CLEANING COMPLETED!\\\")\n",
    "print(f\\\"‚è±Ô∏è  Total time: {elapsed_time:.1f} seconds\\\")\n",
    "print(f\\\"üìä Processed: {processed_count} articles\\\")\n",
    "print(f\\\"‚è≠Ô∏è  Skipped (no entities): {skipped_count}\\\")\n",
    "print(f\\\"‚ùå Errors: {errors_count}\\\")\n",
    "print(f\\\"üèõÔ∏è  Gov entities cleaned: {len(gov_entities_cleaned)}\\\")\n",
    "print(f\\\"‚ö° Rate: {processed_count/elapsed_time:.1f} articles/second\\\")\n",
    "\n",
    "# Create DataFrame\n",
    "gov_entities_cleaned_df = pd.DataFrame(gov_entities_cleaned, columns=[\\\"row_id\\\", \\\"entity_text\\\", \\\"context\\\"]) if gov_entities_cleaned else pd.DataFrame()\n",
    "print(f\\\"üìã Created DataFrame with {len(gov_entities_cleaned_df)} rows\\\")\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b387f944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ OPTIMIZED: Law Mentions Cleaning Only (Phase 1B)\n",
    "print(\"‚öñÔ∏è Starting LAW MENTIONS cleaning (Phase 1B)...\")\n",
    "\n",
    "# Configuration\n",
    "BATCH_SIZE = 100  # Change to None for full processing\n",
    "\n",
    "# Get articles to process (same as government entities)\n",
    "if BATCH_SIZE:\n",
    "    unique_row_ids = identifiers_df['row_id'].unique()[:BATCH_SIZE]\n",
    "    print(f\"üß™ TESTING MODE: Processing first {BATCH_SIZE} articles\")\n",
    "else:\n",
    "    unique_row_ids = identifiers_df['row_id'].unique()\n",
    "    print(f\"üè≠ PRODUCTION MODE: Processing all {len(unique_row_ids)} articles\")\n",
    "\n",
    "# Initialize collectors\n",
    "law_mentions_cleaned = []\n",
    "processed_count = 0\n",
    "errors_count = 0\n",
    "skipped_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"üìä Articles to process: {len(unique_row_ids)}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, row_id in enumerate(unique_row_ids):\n",
    "    processed_count += 1\n",
    "    \n",
    "    # Progress indicator\n",
    "    if processed_count % 10 == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        rate = processed_count / elapsed if elapsed > 0 else 0\n",
    "        eta = (len(unique_row_ids) - processed_count) / rate if rate > 0 else 0\n",
    "        print(f\"üìà Progress: {processed_count}/{len(unique_row_ids)} | Rate: {rate:.1f}/s | ETA: {eta:.0f}s\")\n",
    "    \n",
    "    try:\n",
    "        # Get article info\n",
    "        article_info = identifiers_df[identifiers_df['row_id'] == row_id].iloc[0]\n",
    "        document_name = article_info['document_name']\n",
    "        section_title = article_info['document_section_title']\n",
    "        text = article_info['text']\n",
    "        \n",
    "        # Get law entities for this article\n",
    "        law_entities_raw = law_entities[law_entities['row_id'] == row_id]\n",
    "        \n",
    "        if len(law_entities_raw) == 0:\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "            \n",
    "        # Deduplicate\n",
    "        law_entities_unique = law_entities_raw.drop_duplicates(subset=['entity_text'], keep='first')\n",
    "        \n",
    "        # Process with GPT\n",
    "        try:\n",
    "            prompt = create_prompt_law_mentions(row_id, document_name, section_title, text, law_entities_unique)\n",
    "            resp = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.1\n",
    "            )\n",
    "            output = resp.choices[0].message.content.strip()\n",
    "            cleaned_results = clean_csv_output(output, 3)\n",
    "            law_mentions_cleaned.extend(cleaned_results)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ùå API Error for {row_id}: {e}\")\n",
    "            errors_count += 1\n",
    "        \n",
    "        # Reduced delay\n",
    "        time.sleep(0.1)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Critical error for {row_id}: {e}\")\n",
    "        errors_count += 1\n",
    "\n",
    "# Results\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"\\n‚úÖ LAW MENTIONS CLEANING COMPLETED!\")\n",
    "print(f\"‚è±Ô∏è  Total time: {elapsed_time:.1f} seconds\")\n",
    "print(f\"üìä Processed: {processed_count} articles\")\n",
    "print(f\"‚è≠Ô∏è  Skipped (no entities): {skipped_count}\")\n",
    "print(f\"‚ùå Errors: {errors_count}\")\n",
    "print(f\"‚öñÔ∏è  Law mentions cleaned: {len(law_mentions_cleaned)}\")\n",
    "print(f\"‚ö° Rate: {processed_count/elapsed_time:.1f} articles/second\")\n",
    "\n",
    "# Create DataFrame\n",
    "law_mentions_cleaned_df = pd.DataFrame(law_mentions_cleaned, columns=[\"row_id\", \"law_mention\", \"context\"]) if law_mentions_cleaned else pd.DataFrame()\n",
    "print(f\"üìã Created DataFrame with {len(law_mentions_cleaned_df)} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566f70a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä COMBINE RESULTS and SAVE\n",
    "print(\"üìã Combining Phase 1A and 1B results...\")\n",
    "\n",
    "# Ensure DataFrames exist\n",
    "if 'gov_entities_cleaned_df' not in locals():\n",
    "    gov_entities_cleaned_df = pd.DataFrame(columns=[\"row_id\", \"entity_text\", \"context\"])\n",
    "    print(\"‚ö†Ô∏è  No government entities DataFrame found\")\n",
    "\n",
    "if 'law_mentions_cleaned_df' not in locals():\n",
    "    law_mentions_cleaned_df = pd.DataFrame(columns=[\"row_id\", \"law_mention\", \"context\"])\n",
    "    print(\"‚ö†Ô∏è  No law mentions DataFrame found\")\n",
    "\n",
    "# Save results\n",
    "if not gov_entities_cleaned_df.empty:\n",
    "    gov_output = '/Users/alexa/Projects/cdmx_kg/data/gov_entities_phase1_cleaned.csv'\n",
    "    gov_entities_cleaned_df.to_csv(gov_output, index=False, encoding='utf-8-sig')\n",
    "    print(f\"‚úÖ Government entities saved: {gov_output}\")\n",
    "    print(f\"   üìä {len(gov_entities_cleaned_df)} rows, {gov_entities_cleaned_df['entity_text'].nunique()} unique entities\")\n",
    "\n",
    "if not law_mentions_cleaned_df.empty:\n",
    "    law_output = '/Users/alexa/Projects/cdmx_kg/data/law_mentions_phase1_cleaned.csv'\n",
    "    law_mentions_cleaned_df.to_csv(law_output, index=False, encoding='utf-8-sig')\n",
    "    print(f\"‚úÖ Law mentions saved: {law_output}\")\n",
    "    print(f\"   üìä {len(law_mentions_cleaned_df)} rows, {law_mentions_cleaned_df['law_mention'].nunique()} unique laws\")\n",
    "\n",
    "# Create combined summary\n",
    "total_entities = len(gov_entities_cleaned_df) + len(law_mentions_cleaned_df)\n",
    "print(f\"\\nüéØ PHASE 1 SUMMARY:\")\n",
    "print(f\"   üèõÔ∏è  Government entities: {len(gov_entities_cleaned_df)}\")\n",
    "print(f\"   ‚öñÔ∏è  Law mentions: {len(law_mentions_cleaned_df)}\")\n",
    "print(f\"   üìä Total cleaned entities: {total_entities}\")\n",
    "\n",
    "if total_entities > 0:\n",
    "    print(f\"\\n‚úÖ Ready for Phase 2: Article-Law mapping!\")\n",
    "    print(f\"üìÅ Input files created for Phase 2 verification\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  No entities found - check your data or increase BATCH_SIZE\")\n",
    "\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db417df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç DIAGNOSTIC: Check what happened\n",
    "print(\"üîç DIAGNOSTIC REPORT:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check if raw lists exist\n",
    "print(\"üìã Raw Data Lists:\")\n",
    "if 'gov_entities_cleaned' in locals():\n",
    "    print(f\"   ‚úÖ gov_entities_cleaned exists: {len(gov_entities_cleaned)} items\")\n",
    "else:\n",
    "    print(\"   ‚ùå gov_entities_cleaned NOT FOUND\")\n",
    "\n",
    "if 'law_mentions_cleaned' in locals():\n",
    "    print(f\"   ‚úÖ law_mentions_cleaned exists: {len(law_mentions_cleaned)} items\")\n",
    "else:\n",
    "    print(\"   ‚ùå law_mentions_cleaned NOT FOUND\")\n",
    "\n",
    "# Check if DataFrames exist\n",
    "print(\"\\nüìä DataFrames:\")\n",
    "if 'gov_entities_cleaned_df' in locals():\n",
    "    print(f\"   ‚úÖ gov_entities_cleaned_df exists: {len(gov_entities_cleaned_df)} rows\")\n",
    "else:\n",
    "    print(\"   ‚ùå gov_entities_cleaned_df NOT FOUND\")\n",
    "\n",
    "if 'law_mentions_cleaned_df' in locals():\n",
    "    print(f\"   ‚úÖ law_mentions_cleaned_df exists: {len(law_mentions_cleaned_df)} rows\")\n",
    "else:\n",
    "    print(\"   ‚ùå law_mentions_cleaned_df NOT FOUND\")\n",
    "\n",
    "# Check source data\n",
    "print(\"\\nüìÅ Source Data:\")\n",
    "print(f\"   üèõÔ∏è  Government entities available: {len(gov_entities)}\")\n",
    "print(f\"   ‚öñÔ∏è  Law entities available: {len(law_entities)}\")\n",
    "print(f\"   üìÑ Articles available: {len(identifiers_df)}\")\n",
    "\n",
    "# Check if we have entities in first 100 articles\n",
    "print(\"\\nüß™ Test Sample Check (first 100 articles):\")\n",
    "test_articles = identifiers_df['row_id'].unique()[:100]\n",
    "gov_in_sample = gov_entities[gov_entities['row_id'].isin(test_articles)]\n",
    "law_in_sample = law_entities[law_entities['row_id'].isin(test_articles)]\n",
    "\n",
    "print(f\"   üèõÔ∏è  Gov entities in first 100 articles: {len(gov_in_sample)}\")\n",
    "print(f\"   ‚öñÔ∏è  Law entities in first 100 articles: {len(law_in_sample)}\")\n",
    "\n",
    "if len(gov_in_sample) > 0:\n",
    "    print(f\"   üìä Articles with gov entities: {gov_in_sample['row_id'].nunique()}\")\n",
    "if len(law_in_sample) > 0:\n",
    "    print(f\"   üìä Articles with law entities: {law_in_sample['row_id'].nunique()}\")\n",
    "\n",
    "print(\"\\nüí° RECOMMENDATION:\")\n",
    "if len(gov_in_sample) == 0 and len(law_in_sample) == 0:\n",
    "    print(\"   üéØ NO ENTITIES in first 100 articles!\")\n",
    "    print(\"   üîß Try: Set BATCH_SIZE = 500 or BATCH_SIZE = None\")\n",
    "elif 'gov_entities_cleaned' not in locals():\n",
    "    print(\"   üéØ You need to RUN Cell 21 (Government Entities Processing)\")\n",
    "    print(\"   üîß Execute: Cell 21 ‚Üí Cell 22 ‚Üí Cell 23\")\n",
    "else:\n",
    "    print(\"   üéØ Data exists but processing may have failed\")\n",
    "    print(\"   üîß Check for errors in Cell 21 and 22\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764f27e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save outputs with correct structure for each prompt\n",
    "print(\"üíæ Saving cleaned results...\")\n",
    "\n",
    "# Save government entities cleaned\n",
    "if gov_entities_cleaned:\n",
    "    gov_entities_df = pd.DataFrame(gov_entities_cleaned, columns=[\"row_id\", \"entity_text\", \"context\"])\n",
    "    gov_entities_output = '/Users/alexa/Projects/cdmx_kg/data/gov_entities_cleaned.csv'\n",
    "    gov_entities_df.to_csv(gov_entities_output, index=False, encoding='utf-8-sig')\n",
    "    print(f\"   ‚úÖ Government entities saved to: {gov_entities_output}\")\n",
    "    print(f\"      üìä Total entities: {len(gov_entities_df)}\")\n",
    "    print(f\"      üìä Unique entities: {gov_entities_df['entity_text'].nunique()}\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  No government entities to save\")\n",
    "\n",
    "# Save law mentions cleaned\n",
    "if law_mentions_cleaned:\n",
    "    law_mentions_df = pd.DataFrame(law_mentions_cleaned, columns=[\"row_id\", \"law_mention\", \"context\"])\n",
    "    law_mentions_output = '/Users/alexa/Projects/cdmx_kg/data/law_mentions_cleaned.csv'\n",
    "    law_mentions_df.to_csv(law_mentions_output, index=False, encoding='utf-8-sig')\n",
    "    print(f\"   ‚úÖ Law mentions saved to: {law_mentions_output}\")\n",
    "    print(f\"      üìä Total mentions: {len(law_mentions_df)}\")\n",
    "    print(f\"      üìä Unique laws: {law_mentions_df['law_mention'].nunique()}\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  No law mentions to save\")\n",
    "\n",
    "# Save article-law mappings\n",
    "if article_law_mappings:\n",
    "    article_law_df = pd.DataFrame(article_law_mappings, columns=[\"row_id\", \"article_mention\", \"law_mention\"])\n",
    "    article_law_output = '/Users/alexa/Projects/cdmx_kg/data/article_law_mappings.csv'\n",
    "    article_law_df.to_csv(article_law_output, index=False, encoding='utf-8-sig')\n",
    "    print(f\"   ‚úÖ Article-law mappings saved to: {article_law_output}\")\n",
    "    print(f\"      üìä Total mappings: {len(article_law_df)}\")\n",
    "    print(f\"      üìä Self-references: {len(article_law_df[article_law_df['law_mention'] == ''])}\")\n",
    "    print(f\"      üìä External references: {len(article_law_df[article_law_df['law_mention'] != ''])}\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  No article-law mappings to save\")\n",
    "\n",
    "print(\"\\nüìã Sample results:\")\n",
    "if gov_entities_cleaned:\n",
    "    print(\"Government entities sample:\")\n",
    "    sample_gov = gov_entities_df.head(3)\n",
    "    for _, row in sample_gov.iterrows():\n",
    "        print(f\"   üèõÔ∏è  {row['row_id']}: {row['entity_text']}\")\n",
    "\n",
    "if law_mentions_cleaned:\n",
    "    print(\"Law mentions sample:\")\n",
    "    sample_law = law_mentions_df.head(3)\n",
    "    for _, row in sample_law.iterrows():\n",
    "        print(f\"   ‚öñÔ∏è  {row['row_id']}: {row['law_mention']}\")\n",
    "\n",
    "if article_law_mappings:\n",
    "    print(\"Article-law mappings sample:\")\n",
    "    sample_articles = article_law_df.head(3)\n",
    "    for _, row in sample_articles.iterrows():\n",
    "        law_text = row['law_mention'] if row['law_mention'] else '(current document)'\n",
    "        print(f\"   üìë {row['row_id']}: Article {row['article_mention']} ‚Üí {law_text}\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üéâ GPT CLEANING PIPELINE COMPLETED!\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932bfbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrames from Phase 1 results (if they don't exist)\n",
    "print(\"üîß Creating/Recreating DataFrames from Phase 1 results...\")\n",
    "\n",
    "# Check if the raw lists exist first\n",
    "if 'gov_entities_cleaned' in locals() and gov_entities_cleaned:\n",
    "    # Convert government entities list to DataFrame\n",
    "    gov_entities_cleaned_df = pd.DataFrame(\n",
    "        gov_entities_cleaned, \n",
    "        columns=[\"row_id\", \"entity_text\", \"context\"]\n",
    "    )\n",
    "    print(f\"‚úÖ Created gov_entities_cleaned_df with {len(gov_entities_cleaned_df)} rows\")\n",
    "else:\n",
    "    # Create empty DataFrame if no data\n",
    "    gov_entities_cleaned_df = pd.DataFrame(columns=[\"row_id\", \"entity_text\", \"context\"])\n",
    "    print(\"‚ö†Ô∏è  No gov_entities_cleaned found, created empty DataFrame\")\n",
    "\n",
    "if 'law_mentions_cleaned' in locals() and law_mentions_cleaned:\n",
    "    # Convert law mentions list to DataFrame  \n",
    "    law_mentions_cleaned_df = pd.DataFrame(\n",
    "        law_mentions_cleaned,\n",
    "        columns=[\"row_id\", \"law_mention\", \"context\"]\n",
    "    )\n",
    "    print(f\"‚úÖ Created law_mentions_cleaned_df with {len(law_mentions_cleaned_df)} rows\")\n",
    "else:\n",
    "    # Create empty DataFrame if no data\n",
    "    law_mentions_cleaned_df = pd.DataFrame(columns=[\"row_id\", \"law_mention\", \"context\"])\n",
    "    print(\"‚ö†Ô∏è  No law_mentions_cleaned found, created empty DataFrame\")\n",
    "\n",
    "print(f\"üìä DataFrames ready:\")\n",
    "print(f\"   üèõÔ∏è  Government entities: {len(gov_entities_cleaned_df)} rows\")\n",
    "print(f\"   ‚öñÔ∏è  Law mentions: {len(law_mentions_cleaned_df)} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094a624d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdmx_kg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
