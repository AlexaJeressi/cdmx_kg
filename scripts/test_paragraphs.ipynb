{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f9059357",
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import fitz  # PyMuPDF\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import layoutparser as lp\n",
        "from layoutparser.elements import Rectangle\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "98ae9167",
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# Configuración\n",
        "# -------------------------\n",
        "RENDER_DPI = 200\n",
        "NUM_COLUMNS_HINT = 1\n",
        "MERGE_VGAP = 15\n",
        "\n",
        "# Regex para detectar encabezados de artículos\n",
        "RE_ARTICULO = re.compile(\n",
        "    r'^(art[íi]culo|art\\.)\\s*([0-9]+(?:\\s*(bis|ter|quater))?)',\n",
        "    re.IGNORECASE\n",
        ")\n",
        "\n",
        "# Layout detection using basic image processing (detectron2 alternative)\n",
        "# Since detectron2 installation failed, we'll use a simpler approach\n",
        "class SimpleLayoutDetector:\n",
        "    def __init__(self):\n",
        "        self.label_map = {0:\"text\",1:\"title\",2:\"list\",3:\"table\",4:\"figure\"}\n",
        "    \n",
        "    def detect(self, image_array):\n",
        "        \"\"\"\n",
        "        Simple layout detection using basic image processing.\n",
        "        This is a fallback when detectron2 is not available.\n",
        "        \"\"\"\n",
        "        import cv2\n",
        "        from layoutparser.elements import TextBlock, Rectangle\n",
        "        \n",
        "        # Convert PIL image to opencv format if needed\n",
        "        if len(image_array.shape) == 3:\n",
        "            gray = cv2.cvtColor(image_array, cv2.COLOR_RGB2GRAY)\n",
        "        else:\n",
        "            gray = image_array\n",
        "            \n",
        "        # Use simple contour detection to find text regions\n",
        "        # Apply threshold\n",
        "        _, thresh = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY_INV)\n",
        "        \n",
        "        # Find contours\n",
        "        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        \n",
        "        blocks = []\n",
        "        h, w = gray.shape\n",
        "        \n",
        "        # If no contours found, create a single block covering the whole page\n",
        "        if not contours:\n",
        "            blocks.append(TextBlock(\n",
        "                block=Rectangle(x_1=0, y_1=0, x_2=w, y_2=h),\n",
        "                type=\"text\",\n",
        "                id=0,\n",
        "                score=0.9\n",
        "            ))\n",
        "        else:\n",
        "            # Filter and process contours\n",
        "            for i, contour in enumerate(contours):\n",
        "                x, y, w_cont, h_cont = cv2.boundingRect(contour)\n",
        "                \n",
        "                # Filter out very small regions\n",
        "                if w_cont > 20 and h_cont > 10:\n",
        "                    blocks.append(TextBlock(\n",
        "                        block=Rectangle(x_1=x, y_1=y, x_2=x+w_cont, y_2=y+h_cont),\n",
        "                        type=\"text\",\n",
        "                        id=i,\n",
        "                        score=0.8\n",
        "                    ))\n",
        "        \n",
        "        # Sort by reading order (top to bottom, left to right)\n",
        "        blocks.sort(key=lambda b: (b.block.y_1, b.block.x_1))\n",
        "        \n",
        "        return blocks\n",
        "\n",
        "# Initialize the simple detector\n",
        "detector = SimpleLayoutDetector()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3273ef39",
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# Clases de datos\n",
        "# -------------------------\n",
        "@dataclass\n",
        "class Parrafo:\n",
        "    texto: str\n",
        "    bbox: tuple  # (x1,y1,x2,y2) en coordenadas PDF\n",
        "\n",
        "@dataclass\n",
        "class Articulo:\n",
        "    numero: str\n",
        "    parrafos: List[Parrafo]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e66bcc3b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# Utilidades\n",
        "# -------------------------\n",
        "def page_to_image(page, dpi=RENDER_DPI):\n",
        "    scale = dpi / 72.0\n",
        "    mat = fitz.Matrix(scale, scale)\n",
        "    pix = page.get_pixmap(matrix=mat, alpha=False)\n",
        "    img = Image.frombytes(\"RGB\", (pix.width, pix.height), pix.samples)\n",
        "    return img, scale\n",
        "\n",
        "def detect_paragraph_blocks(pil_img: Image.Image, num_columns_hint=NUM_COLUMNS_HINT):\n",
        "    layout = detector.detect(np.asarray(pil_img))\n",
        "    blocks = [b for b in layout if b.type in (\"text\", \"title\")]\n",
        "    w, h = pil_img.size\n",
        "\n",
        "    # Orden de lectura\n",
        "    if num_columns_hint <= 1:\n",
        "        blocks.sort(key=lambda b: (b.block.y_1, b.block.x_1))\n",
        "    else:\n",
        "        col_w = w / num_columns_hint\n",
        "        cols = {}\n",
        "        for b in blocks:\n",
        "            cx = (b.block.x_1 + b.block.x_2) / 2\n",
        "            idx = min(int(cx // col_w), num_columns_hint-1)\n",
        "            cols.setdefault(idx, []).append(b)\n",
        "        blocks = sum([sorted(c, key=lambda b: (b.block.y_1, b.block.x_1))\n",
        "                      for _, c in sorted(cols.items())], [])\n",
        "\n",
        "    # Fusionar verticalmente bloques cercanos\n",
        "    merged = []\n",
        "    for b in blocks:\n",
        "        if not merged:\n",
        "            merged.append(b); continue\n",
        "        prev = merged[-1]\n",
        "        same_col = abs(prev.block.x_1 - b.block.x_1) < 20 and abs(prev.block.x_2 - b.block.x_2) < 20\n",
        "        close_v = (b.block.y_1 - prev.block.y_2) <= MERGE_VGAP\n",
        "        if same_col and close_v:\n",
        "            new_rect = Rectangle(\n",
        "                x_1=min(prev.block.x_1, b.block.x_1),\n",
        "                y_1=min(prev.block.y_1, b.block.y_1),\n",
        "                x_2=max(prev.block.x_2, b.block.x_2),\n",
        "                y_2=max(prev.block.y_2, b.block.y_2),\n",
        "            )\n",
        "            merged[-1] = lp.TextBlock(block=new_rect, type=prev.type, id=prev.id, score=max(prev.score, b.score))\n",
        "        else:\n",
        "            merged.append(b)\n",
        "    return merged\n",
        "\n",
        "def extract_text_by_bbox(pdf_page: fitz.Page, bbox_img_space: Rectangle, scale: float) -> str:\n",
        "    rect_pdf = fitz.Rect(\n",
        "        bbox_img_space.x_1 / scale,\n",
        "        bbox_img_space.y_1 / scale,\n",
        "        bbox_img_space.x_2 / scale,\n",
        "        bbox_img_space.y_2 / scale,\n",
        "    )\n",
        "    txt = pdf_page.get_textbox(rect_pdf)\n",
        "    txt = txt.replace('\\r\\n', '\\n').replace('\\r', '\\n').strip()\n",
        "    # Limpieza básica: unir líneas que no terminan en puntuación fuerte\n",
        "    lines = [ln.strip() for ln in txt.split(\"\\n\") if ln.strip()]\n",
        "    merged = []\n",
        "    for ln in lines:\n",
        "        if merged and not merged[-1].endswith(tuple(\".;:?!)]\")):\n",
        "            merged[-1] = (merged[-1] + \" \" + ln).strip()\n",
        "        else:\n",
        "            merged.append(ln)\n",
        "    return \" \".join(merged).strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "261d5361",
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# Flujo principal\n",
        "# -------------------------\n",
        "def extract_paragraphs(pdf_path: str) -> List[Parrafo]:\n",
        "    doc = fitz.open(pdf_path)\n",
        "    all_parrafos: List[Parrafo] = []\n",
        "    for i, page in enumerate(doc, start=1):\n",
        "        pil_img, scale = page_to_image(page, dpi=RENDER_DPI)\n",
        "        blocks = detect_paragraph_blocks(pil_img, num_columns_hint=NUM_COLUMNS_HINT)\n",
        "        for b in blocks:\n",
        "            txt = extract_text_by_bbox(page, b.block, scale)\n",
        "            if txt:\n",
        "                rect_pdf = (\n",
        "                    b.block.x_1/scale, b.block.y_1/scale,\n",
        "                    b.block.x_2/scale, b.block.y_2/scale\n",
        "                )\n",
        "                all_parrafos.append(Parrafo(texto=txt, bbox=rect_pdf))\n",
        "    return all_parrafos\n",
        "\n",
        "def group_into_articulos(parrafos: List[Parrafo]) -> List[Articulo]:\n",
        "    articulos: List[Articulo] = []\n",
        "    current_art = None\n",
        "\n",
        "    for p in parrafos:\n",
        "        m = RE_ARTICULO.match(p.texto)\n",
        "        if m:\n",
        "            # Nuevo artículo\n",
        "            num = m.group(2)\n",
        "            current_art = Articulo(numero=num, parrafos=[p])\n",
        "            articulos.append(current_art)\n",
        "        elif current_art is not None:\n",
        "            current_art.parrafos.append(p)\n",
        "        else:\n",
        "            # Texto previo a primer artículo (puede ser considerandos, etc.)\n",
        "            continue\n",
        "    return articulos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d72917cf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# Funciones para guardar resultados\n",
        "# -------------------------\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "def save_results_to_files(articulos: List[Articulo], pdf_name: str, output_dir: str = \"../data/extracted\"):\n",
        "    \"\"\"\n",
        "    Guarda los resultados extraídos en múltiples formatos.\n",
        "    \n",
        "    Args:\n",
        "        articulos: Lista de artículos extraídos\n",
        "        pdf_name: Nombre del archivo PDF procesado\n",
        "        output_dir: Directorio donde guardar los archivos\n",
        "    \"\"\"\n",
        "    # Crear directorio de salida si no existe\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    # Timestamp para archivos únicos\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    base_name = f\"{pdf_name}_{timestamp}\"\n",
        "    \n",
        "    # 1. Guardar como JSON estructurado\n",
        "    json_data = {\n",
        "        \"metadata\": {\n",
        "            \"source_pdf\": pdf_name,\n",
        "            \"extraction_date\": datetime.now().isoformat(),\n",
        "            \"total_articles\": len(articulos),\n",
        "            \"total_paragraphs\": sum(len(art.parrafos) for art in articulos)\n",
        "        },\n",
        "        \"articles\": []\n",
        "    }\n",
        "    \n",
        "    for art in articulos:\n",
        "        article_data = {\n",
        "            \"numero\": art.numero,\n",
        "            \"paragraphs\": []\n",
        "        }\n",
        "        for i, p in enumerate(art.parrafos):\n",
        "            article_data[\"paragraphs\"].append({\n",
        "                \"paragraph_id\": i + 1,\n",
        "                \"text\": p.texto,\n",
        "                \"bbox\": p.bbox\n",
        "            })\n",
        "        json_data[\"articles\"].append(article_data)\n",
        "    \n",
        "    json_path = os.path.join(output_dir, f\"{base_name}.json\")\n",
        "    with open(json_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(json_data, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"✓ JSON guardado: {json_path}\")\n",
        "    \n",
        "    # 2. Guardar como CSV para análisis\n",
        "    rows = []\n",
        "    for art in articulos:\n",
        "        for i, p in enumerate(art.parrafos):\n",
        "            rows.append({\n",
        "                'article_number': art.numero,\n",
        "                'paragraph_id': i + 1,\n",
        "                'text': p.texto,\n",
        "                'bbox_x1': p.bbox[0],\n",
        "                'bbox_y1': p.bbox[1],\n",
        "                'bbox_x2': p.bbox[2],\n",
        "                'bbox_y2': p.bbox[3],\n",
        "                'text_length': len(p.texto)\n",
        "            })\n",
        "    \n",
        "    if rows:\n",
        "        df = pd.DataFrame(rows)\n",
        "        csv_path = os.path.join(output_dir, f\"{base_name}.csv\")\n",
        "        df.to_csv(csv_path, index=False, encoding='utf-8')\n",
        "        print(f\"✓ CSV guardado: {csv_path}\")\n",
        "    \n",
        "    # 3. Guardar resumen en texto plano\n",
        "    txt_path = os.path.join(output_dir, f\"{base_name}_summary.txt\")\n",
        "    with open(txt_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(f\"RESUMEN DE EXTRACCIÓN\\n\")\n",
        "        f.write(f\"==================\\n\\n\")\n",
        "        f.write(f\"Archivo fuente: {pdf_name}\\n\")\n",
        "        f.write(f\"Fecha de extracción: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "        f.write(f\"Total de artículos: {len(articulos)}\\n\")\n",
        "        f.write(f\"Total de párrafos: {sum(len(art.parrafos) for art in articulos)}\\n\\n\")\n",
        "        \n",
        "        for art in articulos:\n",
        "            f.write(f\"ARTÍCULO {art.numero}\\n\")\n",
        "            f.write(f\"{'='*50}\\n\")\n",
        "            for i, p in enumerate(art.parrafos, 1):\n",
        "                f.write(f\"[{i}] {p.texto}\\n\\n\")\n",
        "            f.write(f\"\\n\")\n",
        "    \n",
        "    print(f\"✓ Resumen guardado: {txt_path}\")\n",
        "    \n",
        "    return {\n",
        "        'json': json_path,\n",
        "        'csv': csv_path,\n",
        "        'summary': txt_path\n",
        "    }\n",
        "\n",
        "def debug_article_detection(parrafos: List[Parrafo], limit: int = 20):\n",
        "    \"\"\"\n",
        "    Función de debug para ver qué párrafos podrían ser artículos.\n",
        "    \"\"\"\n",
        "    print(f\"\\n🔍 DEBUGGING: Primeros {limit} párrafos para detectar artículos:\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    for i, p in enumerate(parrafos[:limit]):\n",
        "        # Probar diferentes patrones\n",
        "        matches = []\n",
        "        \n",
        "        # Patrón actual\n",
        "        if RE_ARTICULO.search(p.texto):\n",
        "            matches.append(\"REGEX_ACTUAL\")\n",
        "        \n",
        "        # Patrones alternativos\n",
        "        if re.search(r'\\bart[íi]culo\\s+\\d+', p.texto, re.IGNORECASE):\n",
        "            matches.append(\"ARTICULO_NUM\")\n",
        "        \n",
        "        if re.search(r'\\bart\\.?\\s*\\d+', p.texto, re.IGNORECASE):\n",
        "            matches.append(\"ART_NUM\")\n",
        "        \n",
        "        if re.search(r'^\\s*\\d+\\s*[.-]', p.texto):\n",
        "            matches.append(\"NUM_DASH\")\n",
        "            \n",
        "        match_str = f\" [{', '.join(matches)}]\" if matches else \"\"\n",
        "        \n",
        "        print(f\"{i+1:2d}: {p.texto[:100]}...{match_str}\")\n",
        "        \n",
        "    print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "beee47e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# Ejemplo de uso con guardado de resultados\n",
        "# -------------------------\n",
        "def process_legal_document_with_save(pdf_path, save_results=True, debug=True):\n",
        "    \"\"\"\n",
        "    Procesa un documento legal PDF, extrae artículos estructurados y guarda los resultados.\n",
        "    \n",
        "    Args:\n",
        "        pdf_path (str): Ruta al archivo PDF\n",
        "        save_results (bool): Si guardar los resultados en archivos\n",
        "        debug (bool): Si mostrar información de debug\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (articulos, saved_files_paths)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
        "        print(f\"Procesando: {pdf_path}\")\n",
        "        \n",
        "        # Extraer párrafos\n",
        "        parrafos = extract_paragraphs(pdf_path)\n",
        "        print(f\"✓ Extraídos {len(parrafos)} párrafos\")\n",
        "        \n",
        "        # Debug: mostrar algunos párrafos para entender el formato\n",
        "        if debug and parrafos:\n",
        "            debug_article_detection(parrafos, limit=15)\n",
        "        \n",
        "        # Agrupar en artículos\n",
        "        articulos = group_into_articulos(parrafos)\n",
        "        print(f\"✓ Identificados {len(articulos)} artículos\")\n",
        "        \n",
        "        # Si no se encontraron artículos, probar con patrones alternativos\n",
        "        if len(articulos) == 0 and debug:\n",
        "            print(\"⚠️  No se encontraron artículos con el patrón actual.\")\n",
        "            print(\"Probando patrones alternativos...\")\n",
        "            \n",
        "            # Probar patrón más simple\n",
        "            alternative_articles = group_into_articulos_alternative(parrafos)\n",
        "            if len(alternative_articles) > 0:\n",
        "                print(f\"✓ Patrón alternativo encontró {len(alternative_articles)} artículos\")\n",
        "                articulos = alternative_articles\n",
        "        \n",
        "        # Guardar resultados si se solicita\n",
        "        saved_paths = {}\n",
        "        if save_results and articulos:\n",
        "            saved_paths = save_results_to_files(articulos, pdf_name)\n",
        "            print(f\"📁 Resultados guardados en {len(saved_paths)} archivos\")\n",
        "        elif save_results:\n",
        "            print(\"⚠️  No se guardaron resultados (no se encontraron artículos)\")\n",
        "        \n",
        "        return articulos, saved_paths\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error procesando {pdf_path}: {e}\")\n",
        "        import traceback\n",
        "        if debug:\n",
        "            traceback.print_exc()\n",
        "        return [], {}\n",
        "\n",
        "def group_into_articulos_alternative(parrafos: List[Parrafo]) -> List[Articulo]:\n",
        "    \"\"\"\n",
        "    Función alternativa para agrupar artículos con patrones más flexibles.\n",
        "    \"\"\"\n",
        "    articulos: List[Articulo] = []\n",
        "    current_art = None\n",
        "\n",
        "    # Patrones alternativos más flexibles\n",
        "    patterns = [\n",
        "        re.compile(r'\\b(art[íi]culo)\\s+(\\d+)', re.IGNORECASE),\n",
        "        re.compile(r'\\b(art\\.?)\\s*(\\d+)', re.IGNORECASE),\n",
        "        re.compile(r'^(\\d+)[\\.\\-\\s]', re.MULTILINE),  # Números al inicio\n",
        "    ]\n",
        "\n",
        "    for p in parrafos:\n",
        "        found_article = False\n",
        "        \n",
        "        for pattern in patterns:\n",
        "            match = pattern.search(p.texto)\n",
        "            if match:\n",
        "                # Extraer número del artículo\n",
        "                if len(match.groups()) >= 2:\n",
        "                    num = match.group(2)\n",
        "                else:\n",
        "                    num = match.group(1) if match.group(1).isdigit() else \"?\"\n",
        "                \n",
        "                current_art = Articulo(numero=num, parrafos=[p])\n",
        "                articulos.append(current_art)\n",
        "                found_article = True\n",
        "                break\n",
        "        \n",
        "        if not found_article and current_art is not None:\n",
        "            current_art.parrafos.append(p)\n",
        "        elif not found_article and current_art is None:\n",
        "            # Texto previo al primer artículo\n",
        "            continue\n",
        "    \n",
        "    return articulos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "473c664e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📄 Encontrados 2 archivos PDF:\n",
            "  1. LEY_DE_EDUCACION_DE_LA_CDMX_3.4.pdf\n",
            "  2. LEY_DE_EDUCACION_DE_LA_CDMX_3.4.pdf\n",
            "\n",
            "🚀 Procesando el primer archivo con guardado automático...\n",
            "============================================================\n",
            "Procesando: /Users/alexa/Projects/cdmx_kg/pdfs/LEY_DE_EDUCACION_DE_LA_CDMX_3.4.pdf\n",
            "✓ Extraídos 251 párrafos\n",
            "\n",
            "🔍 DEBUGGING: Primeros 15 párrafos para detectar artículos:\n",
            "============================================================\n",
            " 1: M...\n",
            " 2: LA...\n",
            " 3: CAD...\n",
            " 4: LA...\n",
            " 5: m...\n",
            " 6: m...\n",
            " 7: CAC...\n",
            " 8: LA...\n",
            " 9: RAL...\n",
            "10: CA...\n",
            "11: LA...\n",
            "12: RA....\n",
            "13: LAU...\n",
            "14: BAU...\n",
            "15: LAT...\n",
            "============================================================\n",
            "✓ Identificados 0 artículos\n",
            "⚠️  No se encontraron artículos con el patrón actual.\n",
            "Probando patrones alternativos...\n",
            "⚠️  No se guardaron resultados (no se encontraron artículos)\n",
            "\n",
            "📊 RESUMEN FINAL:\n",
            "✓ Artículos extraídos: 0\n",
            "✓ Total de párrafos: 0\n",
            "\n",
            "⚠️  No se pudieron extraer artículos del documento.\n",
            "Esto puede deberse a:\n",
            "• Formato de documento no estándar\n",
            "• Patrones de artículos diferentes\n",
            "• Problemas en la detección de layout\n"
          ]
        }
      ],
      "source": [
        "# -------------------------\n",
        "# EJECUCIÓN PRINCIPAL CON GUARDADO\n",
        "# -------------------------\n",
        "\n",
        "# Configuración\n",
        "SAVE_RESULTS = True  # Cambiar a False si no quieres guardar archivos\n",
        "DEBUG_MODE = True    # Mostrar información de debug\n",
        "\n",
        "# Buscar archivos PDF en la carpeta del proyecto\n",
        "project_root = \"/Users/alexa/Projects/cdmx_kg\"\n",
        "pdf_files = []\n",
        "\n",
        "# Buscar en múltiples ubicaciones\n",
        "search_paths = [\n",
        "    os.path.join(project_root, \"pdfs\"),\n",
        "    os.path.join(project_root, \"Mexico_City\", \"laws\"),\n",
        "    os.path.join(project_root, \"Mexico_City\", \"laws_1\"),\n",
        "    project_root\n",
        "]\n",
        "\n",
        "for search_path in search_paths:\n",
        "    if os.path.exists(search_path):\n",
        "        for root, dirs, files in os.walk(search_path):\n",
        "            for file in files:\n",
        "                if file.endswith('.pdf'):\n",
        "                    pdf_files.append(os.path.join(root, file))\n",
        "\n",
        "if pdf_files:\n",
        "    print(f\"📄 Encontrados {len(pdf_files)} archivos PDF:\")\n",
        "    for i, pdf in enumerate(pdf_files[:5], 1):  # Mostrar primeros 5\n",
        "        print(f\"  {i}. {os.path.basename(pdf)}\")\n",
        "    if len(pdf_files) > 5:\n",
        "        print(f\"  ... y {len(pdf_files) - 5} más\")\n",
        "    \n",
        "    print(f\"\\n🚀 Procesando el primer archivo con guardado automático...\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Procesar el primer PDF con guardado\n",
        "    articulos, saved_files = process_legal_document_with_save(\n",
        "        pdf_files[0], \n",
        "        save_results=SAVE_RESULTS,\n",
        "        debug=DEBUG_MODE\n",
        "    )\n",
        "    \n",
        "    # Mostrar resumen de resultados\n",
        "    print(f\"\\n📊 RESUMEN FINAL:\")\n",
        "    print(f\"✓ Artículos extraídos: {len(articulos)}\")\n",
        "    print(f\"✓ Total de párrafos: {sum(len(art.parrafos) for art in articulos)}\")\n",
        "    \n",
        "    if saved_files:\n",
        "        print(f\"\\n📁 Archivos guardados:\")\n",
        "        for file_type, path in saved_files.items():\n",
        "            print(f\"  • {file_type.upper()}: {path}\")\n",
        "    \n",
        "    # Mostrar preview de los primeros artículos\n",
        "    if articulos:\n",
        "        print(f\"\\n📖 PREVIEW (primeros 3 artículos):\")\n",
        "        print(\"=\"*60)\n",
        "        for a in articulos[:3]:\n",
        "            print(f\"\\n🔹 ARTÍCULO {a.numero} ({len(a.parrafos)} párrafos)\")\n",
        "            for i, p in enumerate(a.parrafos[:2], 1):  # Solo primeros 2 párrafos por artículo\n",
        "                text_preview = p.texto[:150] + '...' if len(p.texto) > 150 else p.texto\n",
        "                print(f\"   [{i}] {text_preview}\")\n",
        "            if len(a.parrafos) > 2:\n",
        "                print(f\"   ... y {len(a.parrafos) - 2} párrafos más\")\n",
        "    else:\n",
        "        print(f\"\\n⚠️  No se pudieron extraer artículos del documento.\")\n",
        "        print(\"Esto puede deberse a:\")\n",
        "        print(\"• Formato de documento no estándar\")\n",
        "        print(\"• Patrones de artículos diferentes\")\n",
        "        print(\"• Problemas en la detección de layout\")\n",
        "        \n",
        "else:\n",
        "    print(\"❌ No se encontraron archivos PDF en el proyecto.\")\n",
        "    print(\"Ubicaciones buscadas:\")\n",
        "    for path in search_paths:\n",
        "        exists = \"✓\" if os.path.exists(path) else \"✗\"\n",
        "        print(f\"  {exists} {path}\")\n",
        "    print(\"\\nPara probar el código, coloca un archivo PDF en alguna de estas carpetas.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cdmx_kg",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
